{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# InClass Kaggle Competition\n",
    "### Nour Azar, Sofie Ghysels, Fernando Delgado\n",
    "\n",
    "---\n",
    "\n",
    "The machine learning pipeline includes:\n",
    "\n",
    "1. Data processing\n",
    "- Error correction\n",
    "- Feature engineering\n",
    "- Value transformation\n",
    "- Variable selection\n",
    "\n",
    "2. Modeling\n",
    "- Logistic Regression, Random Forest, XGBoost, KNN, SVM\n",
    "- Hyper parameter tuning\n",
    "\n",
    "3. Experimental setup\n",
    "- k-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "# MODIFY THESE FLAGS TO TURN ON/OFF THE DATA PROCESSING FUNCTIONS #\n",
    "###################################################################\n",
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Feature engineering step\n",
    "enable_num_poly = True  # Add polynomial terms\n",
    "\n",
    "# Value transformation step\n",
    "enable_trans_cat_dt = True  # Remapping cat variables - Decision tree–based remapping\n",
    "enable_trans_num_dt = True  # Discretizing num variables - Decision tree–based discretization\n",
    "enable_trans_num_ef = False # Equal Freq Discretization\n",
    "enable_trans_num_ew = False # Equal Weight Discretization \n",
    "\n",
    "#Other\n",
    "fixpoutcome = True\n",
    "nofixpoutcome = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"C:/Users/fdelgado/OneDrive - IESEG/Documents/01. IESEG/13. Statistical and Machine Learning Approaches/Group_Project_Group6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder,  KBinsDiscretizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import mutual_info_classif, VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "xgb.set_config(verbosity=0) #Fix warning error from xgb\n",
    "\n",
    "#Scores\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Experimental setup\n",
    "from sklearn.model_selection import KFold,  RepeatedStratifiedKFold, cross_validate, GridSearchCV\n",
    "\n",
    "#Other\n",
    "import shutup; shutup.please() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Read and print out some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train, test\n",
    "train = pd.read_csv(wd + './data/raw/bank_mkt_train.csv', low_memory=False)\n",
    "test = pd.read_csv(wd + './data/raw/bank_mkt_test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object     10\n",
      "float64     9\n",
      "int64       2\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>subscribe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29925</td>\n",
       "      <td>42.0</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>jul</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.918</td>\n",
       "      <td>-42.7</td>\n",
       "      <td>4.968</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37529</td>\n",
       "      <td>35.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>married</td>\n",
       "      <td>university.degree</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>jun</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.960</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2757</td>\n",
       "      <td>44.0</td>\n",
       "      <td>technician</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.9y</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>cellular</td>\n",
       "      <td>may</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>92.893</td>\n",
       "      <td>-46.2</td>\n",
       "      <td>1.264</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642</td>\n",
       "      <td>45.0</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>cellular</td>\n",
       "      <td>apr</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>93.075</td>\n",
       "      <td>-47.1</td>\n",
       "      <td>1.453</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14183</td>\n",
       "      <td>45.0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>married</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.859</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   client_id   age         job  marital          education  default  housing  \\\n",
       "0      29925  42.0  management  married           basic.9y       no       no   \n",
       "1      37529  35.0  unemployed  married  university.degree       no      yes   \n",
       "2       2757  44.0  technician  married           basic.9y       no      yes   \n",
       "3       9642  45.0    services  married        high.school       no      yes   \n",
       "4      14183  45.0     unknown  married            unknown  unknown  unknown   \n",
       "\n",
       "      loan    contact month  ... campaign  pdays  previous     poutcome  \\\n",
       "0       no   cellular   jul  ...      1.0  999.0       0.0  nonexistent   \n",
       "1       no  telephone   jun  ...      4.0  999.0       0.0  nonexistent   \n",
       "2      yes   cellular   may  ...      1.0  999.0       0.0  nonexistent   \n",
       "3       no   cellular   apr  ...      1.0  999.0       0.0  nonexistent   \n",
       "4  unknown  telephone   may  ...      1.0  999.0       0.0  nonexistent   \n",
       "\n",
       "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed  \\\n",
       "0          1.4          93.918          -42.7      4.968       5228.1   \n",
       "1          1.4          94.465          -41.8      4.960       5228.1   \n",
       "2         -1.8          92.893          -46.2      1.264       5099.1   \n",
       "3         -1.8          93.075          -47.1      1.453       5099.1   \n",
       "4          1.1          93.994          -36.4      4.859       5191.0   \n",
       "\n",
       "   subscribe  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out to check the data\n",
    "print(train.dtypes.value_counts())\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Create a list of column names to manage variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# General list of variables\n",
    "id_var = [\"client_id\"]  # ID\n",
    "target_var = [\"subscribe\"]  # Target get variable\n",
    "predictors = [v for v in train.columns if v not in id_var + target_var]\n",
    "\n",
    "# List of numerical and catergorical variables\n",
    "num_vars = ['age', 'campaign', 'pdays', 'previous',\n",
    "            'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']\n",
    "cat_vars = ['job', 'marital', 'education', 'default', 'housing', 'loan',\n",
    "            'contact', 'month', 'day_of_week',\n",
    "            'poutcome']\n",
    "\n",
    "# Double check the list of variables\n",
    "assert(len(predictors) == len(num_vars) + len(cat_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Check the target variable class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subscribe\n",
      "0            17729\n",
      "1             2271\n",
      "dtype: int64\n",
      "subscribe\n",
      "0            0.88645\n",
      "1            0.11355\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# By number\n",
    "print(train[target_var].value_counts())\n",
    "\n",
    "# By percentage\n",
    "print(train[target_var].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Create a Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[predictors], \n",
    "    train[target_var], \n",
    "    test_size = 0.2, \n",
    "    stratify = train[target_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Error, data correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Check and correct data error - Constant variables\n",
    "\n",
    "Constant variables on train do not contain information and may cause data processing or model training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop constant variable: []\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "\n",
    "# Count number of unique values of each variable\n",
    "vars_nunique = train[num_vars + cat_vars].apply(pd.Series.nunique, dropna=False, axis=0)\n",
    "cont_vars = vars_nunique.index[vars_nunique < 2].tolist()\n",
    "print(\"Drop constant variable:\", cont_vars)\n",
    "\n",
    "# Correct variable list\n",
    "num_vars = [v for v in num_vars if v not in cont_vars]\n",
    "cat_vars = [v for v in cat_vars if v not in cont_vars]\n",
    "\n",
    "# Update train, test\n",
    "X_train = X_train[num_vars + cat_vars]\n",
    "X_test = X_test[num_vars + cat_vars]\n",
    "test = test[id_var + num_vars + cat_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Check and correct data error - Missing values\n",
    "\n",
    "<u>Note:</u>\n",
    "- Always create indicators (dummy variable) to track the missing values imputation.\n",
    "- Since we already filtered out the constant NA vars, the imputor will not drop any vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - # NA of num vars: 1391\n",
      "Train - # NA of cat vars: 1547\n",
      "Validation - # NA of num vars: 339\n",
      "Validation - # NA of cat vars: 387\n",
      "Test - # NA of num vars: 918\n",
      "Test - # NA of cat vars: 1057\n"
     ]
    }
   ],
   "source": [
    "# Check missing value\n",
    "print('Train - # NA of num vars:', X_train[num_vars].isna().sum().sum())\n",
    "print('Train - # NA of cat vars:', X_train[cat_vars].isna().sum().sum())\n",
    "print('Validation - # NA of num vars:', X_test[num_vars].isna().sum().sum())\n",
    "print('Validation - # NA of cat vars:', X_test[cat_vars].isna().sum().sum())\n",
    "print('Test - # NA of num vars:', test[num_vars].isna().sum().sum())\n",
    "print('Test - # NA of cat vars:', test[cat_vars].isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop num variables with high missing pct: []\n",
      "Drop cat variables with high missing pct: []\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "\n",
    "# Here, we test the effect of dropping variables with high missing percentage (>30%)\n",
    "na_threshold = 0.3\n",
    "\n",
    "# Drop num variables with more than 30% missing values\n",
    "num_na_pct = X_train[num_vars].isnull().mean()\n",
    "num_vars = num_na_pct[num_na_pct <= na_threshold].index.tolist()\n",
    "print(\"Drop num variables with high missing pct:\", num_na_pct[num_na_pct > na_threshold].tolist())\n",
    "\n",
    "# Drop cat variables with more than 30% missing values\n",
    "cat_na_pct = X_train[cat_vars].isnull().mean()\n",
    "cat_vars = cat_na_pct[cat_na_pct <= 0.3].index.tolist()\n",
    "print(\"Drop cat variables with high missing pct:\", cat_na_pct[cat_na_pct > na_threshold].tolist())\n",
    "\n",
    "# Update train, test\n",
    "X_train = X_train[num_vars + cat_vars]\n",
    "X_test = X_test[num_vars + cat_vars]\n",
    "test = test[id_var + num_vars + cat_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Fix Categoricals NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fixpoutcome:\n",
    "    #Fix poutcome nonexistent to unknown\n",
    "    X_train['poutcome'] = np.where(X_train['poutcome'] == 'nonexistent', 'unknown', X_train['poutcome'] )\n",
    "    X_test['poutcome'] = np.where(X_test['poutcome'] == 'nonexistent', 'unknown', X_test['poutcome'] )\n",
    "    test['poutcome'] = np.where(test['poutcome'] == 'nonexistent', 'unknown', test['poutcome'] )\n",
    "\n",
    "    #Fill categoricals with \"unknown\"\n",
    "    for c in cat_vars:\n",
    "        X_train[c].fillna(value='unknown', inplace=True)\n",
    "        X_test[c].fillna(value='unknown', inplace=True)\n",
    "        test[c].fillna(value='unknown', inplace=True)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nofixpoutcome:\n",
    "    #Fill categoricals with \"unknown\"\n",
    "    for c in cat_vars:\n",
    "        X_train[c].fillna(value='unknown', inplace=True)\n",
    "        X_test[c].fillna(value='unknown', inplace=True)\n",
    "        test[c].fillna(value='unknown', inplace=True)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Fix Numericals NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fix AGE with the mean  \n",
    "age_mean = X_train['age'].mean()\n",
    "\n",
    "#Train\n",
    "X_train['missing_age'] = np.where(X_train['age'].isnull(), 1, 0)\n",
    "X_train['age'].fillna(value=age_mean, inplace=True)\n",
    "\n",
    "#Validation\n",
    "X_test['missing_age'] = np.where(X_test['age'].isnull(), 1, 0)\n",
    "X_test['age'].fillna(value=age_mean, inplace=True)\n",
    "\n",
    "#Test\n",
    "test['missing_age'] = np.where(test['age'].isnull(), 1, 0)\n",
    "test['age'].fillna(value=age_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill other numericals with mode\n",
    "# The reasoning behind this is because they are money-related variables, not really continuous variables.\n",
    "\n",
    "for c in num_vars:\n",
    "    if X_train[c].isna().sum() > 0:\n",
    "        mode = X_train[c].mode()[0]\n",
    "        #Tracking var\n",
    "        X_train['missing_' + str(c)] = np.where(X_train[c].isnull(), 1, 0)\n",
    "        X_test['missing_' + str(c)] = np.where(X_test[c].isnull(), 1, 0)\n",
    "        test['missing_' + str(c)] = np.where(test[c].isnull(), 1, 0)\n",
    "        #Fill NA\n",
    "        X_train[c].fillna(value=mode, inplace=True)\n",
    "        X_test[c].fillna(value=mode, inplace=True)\n",
    "        test[c].fillna(value=mode, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remake column list\n",
    "na_vars = [col for col in X_train.columns if 'missing' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Check and correct data error - Outliers in numerical variables\n",
    "\n",
    "Here, we only check but not impute the outliers.\n",
    "\n",
    "<u>Note:</u>\n",
    "- Before correcting outliers, make sure to understand the nature of the error.\n",
    "- Do not need to correct outliers for missing values indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age has # outliers on train, validation, test : 139 [ 0.87 % ] 42 [ 1.05 % ] 89 [ 0.89 % ]\n",
      "campaign has # outliers on train, validation, test : 339 [ 2.12 % ] 84 [ 2.1 % ] 210 [ 2.1 % ]\n",
      "pdays has # outliers on train, validation, test : 584 [ 3.65 % ] 166 [ 4.15 % ] 371 [ 3.71 % ]\n",
      "previous has # outliers on train, validation, test : 404 [ 2.53 % ] 115 [ 2.88 % ] 269 [ 2.69 % ]\n",
      "Wall time: 74 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Check the outliers on train, test\n",
    "for v in num_vars:\n",
    "    # Calculate the boundaries on train [mean-3*sd, mean+3*sd]\n",
    "    mu = np.mean(X_train[v])\n",
    "    sd = np.std(X_train[v])\n",
    "    lower = mu - 3*sd\n",
    "    upper = mu + 3*sd\n",
    "    # Check outliers using the boundaries\n",
    "    train_out = (X_train[v] < lower) | (X_train[v] > upper)\n",
    "    val_out = (X_test[v] < lower) | (X_test[v] > upper)\n",
    "    test_out = (test[v] < lower) | (test[v] > upper)\n",
    "    if np.sum(train_out) + np.sum(val_out) + np.sum(test_out) > 0:\n",
    "        print(v, \"has # outliers on train, validation, test :\",\n",
    "              np.sum(train_out), \"[\", np.round(100*np.mean(train_out), 2), \"% ]\",\n",
    "              np.sum(val_out), \"[\", np.round(100*np.mean(val_out), 2), \"% ]\",\n",
    "              np.sum(test_out), \"[\", np.round(100*np.mean(test_out), 2), \"% ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) Encode categorical variables\n",
    "\n",
    "In Python, most of the machine learning libraries will not handle non-numerical values of categorical varibales (e.g. RF). Therefore, we should encoded the categories using integer values.\n",
    "\n",
    "<u>Note:</u>\n",
    "- Here, the encoder is fitted on both train and test. Therefore, in a few special cases, there are unique categories that appear only on train, or test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Encode categorical variables as integer values\n",
    "\n",
    "# Categorical variables in any format will be converted to string\n",
    "# Note: All the NA values were imputed previously\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(pd.concat([X_train[cat_vars].astype(str), X_test[cat_vars].astype(str), test[cat_vars].astype(str)], axis=0))\n",
    "\n",
    "# Apply on train, test\n",
    "X_train[cat_vars] = enc.transform(X_train[cat_vars].astype(str))\n",
    "X_test[cat_vars] = enc.transform(X_test[cat_vars].astype(str))\n",
    "test[cat_vars] = enc.transform(test[cat_vars].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (g) Finalize the processed data\n",
    "\n",
    "Current lists of variables:\n",
    "- id_var : customer ID\n",
    "- num_vars : numerical variables\n",
    "- cat_vars : categorical variables\n",
    "- na_vars : indicators for tracking missing values, bool [False, True]\n",
    "- target_var : target variable, churn [0, 1]\n",
    "\n",
    "<u>Note:</u> If there are any variables exist in only train or test, drop them from all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bool variable to int\n",
    "X_train[na_vars] = X_train[na_vars].astype(np.int8)\n",
    "X_test[na_vars] = X_test[na_vars].astype(np.int8)\n",
    "test[na_vars] = test[na_vars].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# id_var [ 1 ] : ['client_id']\n",
      "# num_vars [ 9 ] : ['age', 'campaign', 'pdays', 'previous', 'emp.var.rate'] ...\n",
      "# cat_vars [ 10 ] : ['job', 'marital', 'education', 'default', 'housing'] ...\n",
      "# na_vars [ 9 ] : ['missing_age', 'missing_campaign', 'missing_pdays', 'missing_previous', 'missing_emp.var.rate'] ...\n",
      "# target_var [ 1 ] : ['subscribe']\n"
     ]
    }
   ],
   "source": [
    "# Print out the final variables\n",
    "print(\"# id_var [\", len(id_var), \"] :\", id_var)\n",
    "print(\"# num_vars [\", len(num_vars), \"] :\", num_vars[:5], \"...\")\n",
    "print(\"# cat_vars [\", len(cat_vars), \"] :\", cat_vars[:5], \"...\")\n",
    "print(\"# na_vars [\", len(na_vars), \"] :\", na_vars[:5], \"...\")\n",
    "print(\"# target_var [\", len(target_var), \"] :\", target_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 29)\n",
      "(4000, 29)\n",
      "(10000, 29)\n"
     ]
    }
   ],
   "source": [
    "# Sort the data according to the variables list\n",
    "X_train = X_train[num_vars + cat_vars + na_vars] \n",
    "X_test = X_test[num_vars + cat_vars + na_vars]\n",
    "test = test[id_var + num_vars + cat_vars + na_vars]\n",
    "\n",
    "# Add target variable back to table\n",
    "X_train['target'] = y_train\n",
    "X_test['target'] = y_test\n",
    "\n",
    "# Assign target variable name\n",
    "target_var = ['target']\n",
    "\n",
    "# Check dimensions\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - # NA:  0\n",
      "Validation - # NA:  0\n",
      "Test - # NA: 0\n"
     ]
    }
   ],
   "source": [
    "# Check missing value\n",
    "print('Train - # NA: ', X_train.isna().sum().sum())\n",
    "print('Validation - # NA: ', X_test.isna().sum().sum())\n",
    "print('Test - # NA:', test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Feature engineering\n",
    "\n",
    "Since every variables are anonymized, we cannot create new variables based the meaning of current variables.\n",
    "\n",
    "In other case, we can use the following hints:\n",
    "- Quickly check the potentially important variables.\n",
    "- Focus on the most important variables to create new variables.\n",
    "- Create a cross variable framework (e.g. customer activities, date-time, events, etc.) for searching the new variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Quickly detect most (potentially) important varriables - Correlation test for numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 vars [+] correlated with target_var :\n",
      "previous         0.231664\n",
      "cons.conf.idx    0.057886\n",
      "age              0.029698\n",
      "Name: target, dtype: float64\n",
      "Top 5 vars [-] correlated with target_var :\n",
      "cons.price.idx   -0.141333\n",
      "emp.var.rate     -0.298751\n",
      "euribor3m        -0.304746\n",
      "pdays            -0.325353\n",
      "nr.employed      -0.349546\n",
      "Name: target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Pearson's correlation for numerical variables\n",
    "corr = X_train[num_vars + target_var].corr(method='pearson')\n",
    "corr = corr[target_var[0]][:-1].dropna().sort_values(ascending=False)\n",
    "print(\"Top 5 vars [+] correlated with target_var :\"); print(corr[corr > 0][:5])\n",
    "print(\"Top 5 vars [-] correlated with target_var :\"); print(corr[corr < 0][-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Quickly detect most (potentially) important varriables - Mutual information\n",
    "\n",
    "<u>Reference:</u>\n",
    "- Mutual information. Link: https://en.wikipedia.org/wiki/Mutual_information\n",
    "- sklearn.feature_selection.mutual_info_classif. Link: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 vars :\n",
      "poutcome       0.031857\n",
      "month          0.029137\n",
      "contact        0.012988\n",
      "job            0.010981\n",
      "default        0.006440\n",
      "education      0.005720\n",
      "day_of_week    0.000476\n",
      "marital        0.000338\n",
      "loan           0.000133\n",
      "housing        0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Check mutual information for all variables\n",
    "# Note: The calculation involves a random process, therefore, the result may change if there is no\n",
    "# variable with significant information.\n",
    "mutual_info = mutual_info_classif(X_train[cat_vars], X_train[target_var].values.squeeze())\n",
    "mutual_info = pd.Series(mutual_info, index=cat_vars)\n",
    "print(\"Top 10 vars :\"); print(mutual_info.sort_values(ascending=False)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable age AUC vs. AUC poly: 0.49184751116825787 --> 0.5875963458026546\n",
      "Variable cons.conf.idx AUC vs. AUC poly: 0.540377880844484 --> 0.5928455230317504\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "if enable_num_poly:\n",
    "    for v in num_vars:\n",
    "        # Setup the LR model\n",
    "        cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "        model = LogisticRegression(max_iter=200)\n",
    "        parameters = {}\n",
    "        clf = GridSearchCV(model, parameters, scoring=\"roc_auc\", n_jobs=-1, cv=cv, verbose=0)\n",
    "        \n",
    "        # Fit the LR model for 1 numerical variable\n",
    "        clf.fit(X_train[[v]], X_train[target_var].squeeze())\n",
    "        clf_num_score = clf.best_score_\n",
    "        \n",
    "        # Fit the LR model for 1 numerical variable + it polynomial degree = 3\n",
    "        poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "        poly.fit(X_train[[v]])\n",
    "        clf.fit(poly.transform(X_train[[v]]), X_train[target_var].squeeze())\n",
    "        clf_poly_score = clf.best_score_\n",
    "        \n",
    "        # Add the polynomial terms to train, test\n",
    "        if (clf_poly_score > 0.5) & (clf_poly_score - clf_num_score > 0.05):\n",
    "            print('Variable', v, 'AUC vs. AUC poly:', clf_num_score, '-->', clf_poly_score)\n",
    "            poly_vars = [v_poly.replace('x0', v) for v_poly in poly.get_feature_names()[1:]]\n",
    "            num_vars = num_vars + poly_vars\n",
    "            X_train[poly_vars] = pd.DataFrame(poly.transform(X_train[[v]])[:, 1:], columns=poly_vars)\n",
    "            X_test[poly_vars] = pd.DataFrame(poly.transform(X_test[[v]])[:, 1:], columns=poly_vars)\n",
    "            test[poly_vars] = pd.DataFrame(poly.transform(test[[v]])[:, 1:], columns=poly_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Fill NAs in poly vars\n",
    "# Poly variables create missing values when they tend to infinite, so we fill those with the mean.\n",
    "# Note: professors code to fill with mean was giving errors\n",
    "poly_vars = [v for v in num_vars if '^' in v]\n",
    "       \n",
    "for v in poly_vars:\n",
    "    mean_value=X_train[v].mean()\n",
    "    X_train[v].fillna(value=mean_value, inplace=True)\n",
    "    X_test[v].fillna(value=mean_value, inplace=True)\n",
    "    test[v].fillna(value=mean_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to track the value transformation process\n",
    "trans_vars = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. Remapping categorical variables - Decision tree–based remapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Test the variable remmaping on a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.6374528347152759\n",
      "Best params: {'min_samples_leaf': 200}\n",
      "Number of leaves: 8\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Select a cat variable\n",
    "v = \"month\"\n",
    "\n",
    "# Find the best decision tree\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier()\n",
    "parameters = {'min_samples_leaf':(train.shape[0]*np.array([0.01, 0.025, 0.05, 0.1, 0.25, 0.5])).astype(int)}\n",
    "clf = GridSearchCV(model, parameters, scoring=\"roc_auc\", n_jobs=-1, cv=cv, verbose=0)\n",
    "clf.fit(X_train[[v]], X_train[target_var])\n",
    "print(\"Best AUC:\", clf.best_score_)\n",
    "print(\"Best params:\", clf.best_params_)\n",
    "print(\"Number of leaves:\", clf.best_estimator_.get_n_leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# original categories: 11\n",
      "# new remapped categories: 8 {3, 6, 7, 8, 10, 11, 13, 14}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Grouping the categories into new categories (leaves) using Decision Tree\n",
    "# Here, we use the decision path, the last node is the new segment of an observation\n",
    "remap_v = [np.nonzero(r)[0].max() for r in clf.best_estimator_.decision_path(X_train[[v]]).toarray()]\n",
    "\n",
    "print(\"# original categories:\", X_train[[v]].nunique().values[0])\n",
    "print(\"# new remapped categories:\", len(set(remap_v)), set(remap_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Apply the variable remmaping for all categorical variables\n",
    "\n",
    "<u>Note:</u> Only remap the variables if AUC > 0.5 and the number of new categories > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remapping variable job from 12 to 12 categories\n",
      "Remapping variable marital from 4 to 3 categories\n",
      "Remapping variable education from 8 to 6 categories\n",
      "Remapping variable default from 3 to 2 categories\n",
      "Remapping variable housing from 3 to 2 categories\n",
      "Remapping variable contact from 3 to 3 categories\n",
      "Remapping variable month from 11 to 9 categories\n",
      "Remapping variable day_of_week from 6 to 2 categories\n",
      "Remapping variable poutcome from 3 to 3 categories\n",
      "Wall time: 2.9 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "if enable_trans_cat_dt:\n",
    "    for v in cat_vars:\n",
    "        # Find the best decision tree using CV\n",
    "        cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "        model = DecisionTreeClassifier()\n",
    "        parameters = {'min_samples_leaf':(X_train.shape[0]*np.array([0.01, 0.025, 0.05, 0.1, 0.25, 0.5])).astype(int)}\n",
    "        clf = GridSearchCV(model, parameters, scoring=\"roc_auc\", n_jobs=-1, cv=cv, verbose=0)\n",
    "        clf.fit(X_train[[v]], X_train[target_var])\n",
    "        # Remap the variable on train, test\n",
    "        if (clf.best_score_ > 0.5) & (clf.best_estimator_.get_n_leaves() > 1):\n",
    "            print(\"Remapping variable\", v,\n",
    "                  \"from\", X_train[[v]].nunique().values[0],\n",
    "                  \"to\", clf.best_estimator_.get_n_leaves(), \"categories\")\n",
    "            remap_var = v + '_remap'\n",
    "            trans_vars.append(remap_var)\n",
    "            X_train[remap_var] = [np.nonzero(r)[0].max() for r in clf.best_estimator_.decision_path(X_train[[v]]).toarray()]\n",
    "            X_test[remap_var] = [np.nonzero(r)[0].max() for r in clf.best_estimator_.decision_path(X_test[[v]]).toarray()]\n",
    "            test[remap_var] = [np.nonzero(r)[0].max() for r in clf.best_estimator_.decision_path(test[[v]]).toarray()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Discretizing (or binning) numerical variables - Decision tree–based discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Test the variable discretizing on a numerical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AUC: 0.6120916841112145\n",
      "Best params: {'min_samples_leaf': 160}\n",
      "Number of leaves: 3\n",
      "Wall time: 133 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Select a num variable\n",
    "v = \"previous\"\n",
    "\n",
    "# Find the best decision tree\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "model = DecisionTreeClassifier()\n",
    "parameters = {'min_samples_leaf':(X_train.shape[0]*np.array([0.01, 0.025, 0.05, 0.1, 0.25, 0.5])).astype(int)}\n",
    "clf = GridSearchCV(model, parameters, scoring=\"roc_auc\", n_jobs=-1, cv=cv, verbose=0)\n",
    "clf.fit(X_train[[v]], X_train[target_var])\n",
    "print(\"Best AUC:\", clf.best_score_)\n",
    "print(\"Best params:\", clf.best_params_)\n",
    "print(\"Number of leaves:\", clf.best_estimator_.get_n_leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# original data range:  [0.0, 6.0]\n",
      "# new remapped categories: 3 {1, 3, 4}\n",
      "Wall time: 77 ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Grouping the categories into new categories (leaves) using Decision Tree\n",
    "# Here, we use the decision path, the last node is the new segment of an observation\n",
    "remap_v = [np.nonzero(r)[0].max() for r in clf.best_estimator_.decision_path(X_train[[v]]).toarray()]\n",
    "\n",
    "print(\"# original data range: \", [X_train[[v]].min().values[0], X_train[[v]].max().values[0]])\n",
    "print(\"# new remapped categories:\", len(set(remap_v)), set(remap_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Apply the variable discretizing for all numerical variables\n",
    "\n",
    "<u>Note:</u> Only bin/discretize the variables if the number of new categories > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretize variable age from [17.0, 98.0] to 26 categories\n",
      "Discretize variable campaign from [1.0, 56.0] to 7 categories\n",
      "Discretize variable pdays from [0.0, 999.0] to 3 categories\n",
      "Discretize variable previous from [0.0, 6.0] to 3 categories\n",
      "Discretize variable emp.var.rate from [-3.4, 1.4] to 8 categories\n",
      "Discretize variable cons.price.idx from [92.201, 94.767] to 14 categories\n",
      "Discretize variable cons.conf.idx from [-50.8, -26.9] to 15 categories\n",
      "Discretize variable euribor3m from [0.634, 5.045] to 47 categories\n",
      "Discretize variable nr.employed from [4963.6, 5228.1] to 9 categories\n",
      "Discretize variable age^2 from [289.0, 9604.0] to 24 categories\n",
      "Discretize variable age^3 from [4913.0, 941192.0] to 23 categories\n",
      "Discretize variable cons.conf.idx^2 from [723.6099999999999, 2580.64] to 5 categories\n",
      "Discretize variable cons.conf.idx^3 from [-131096.512, -19465.108999999997] to 5 categories\n",
      "Wall time: 4.9 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "if enable_trans_num_dt:\n",
    "    for v in num_vars:\n",
    "        # Find the best decision tree using CV\n",
    "        cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "        model = DecisionTreeClassifier()\n",
    "        parameters = {'min_samples_leaf':(X_train.shape[0]*np.array([0.01, 0.025, 0.05, 0.1, 0.25, 0.5])).astype(int)}\n",
    "        clf = GridSearchCV(model, parameters, scoring=\"roc_auc\", n_jobs=-1, cv=cv, verbose=0)\n",
    "        clf.fit(X_train[[v]], X_train[target_var])\n",
    "        # Remap the variable on train, test\n",
    "        if (clf.best_score_ > 0.5) & (clf.best_estimator_.get_n_leaves() > 1):\n",
    "            print(\"Discretize variable\", v,\n",
    "                  \"from\", [X_train[[v]].min().values[0], X_train[[v]].max().values[0]],\n",
    "                  \"to\", clf.best_estimator_.get_n_leaves(), \"categories\")\n",
    "            remap_var = v + '_bin'\n",
    "            trans_vars.append(remap_var)\n",
    "            X_train[remap_var] = [np.nonzero(r)[0].max() for r in clf.best_estimator_.decision_path(X_train[[v]]).toarray()]\n",
    "            X_test[remap_var] = [np.nonzero(r)[0].max() for r in clf.best_estimator_.decision_path(X_test[[v]]).toarray()]\n",
    "            test[remap_var] = [np.nonzero(r)[0].max() for r in clf.best_estimator_.decision_path(test[[v]]).toarray()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3. Discretizing (or binning) numerical variables - Equal frequency discretization\n",
    "\n",
    "<u>Note:</u>  \n",
    "- According to Coussement, Lessmann, & Verstraeten (2017), the number of bins at 10 is optimal. However, we can also test with different values in the range [5, 10]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins : 10\n",
      "Wall time: 10 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([array([17., 28., 31., 33., 36., 38., 41., 45., 49., 55., 98.])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Select a num variable\n",
    "v = \"age\"\n",
    "\n",
    "# Binning values of a variable into new groups using equal frequency approach\n",
    "est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "est.fit(X_train[[v]])\n",
    "print(\"Number of bins :\", est.n_bins_[0])\n",
    "est.bin_edges_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0    1257\n",
       "1.0    1577\n",
       "2.0    1432\n",
       "3.0    2024\n",
       "4.0    1291\n",
       "5.0    1719\n",
       "6.0    1731\n",
       "7.0    1538\n",
       "8.0    1806\n",
       "9.0    1625\n",
       "dtype: int64"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Apply the binning to a variable\n",
    "binef_v = est.transform(X_train[[v]])\n",
    "pd.DataFrame(binef_v).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Apply the variable discretizing for all numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "if enable_trans_num_ef:\n",
    "    for v in num_vars:\n",
    "        # Binning values of a variable\n",
    "        est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n",
    "        est.fit(X_train[[v]])\n",
    "        # Bin the variable on train, test\n",
    "        if est.n_bins_[0] > 1:\n",
    "            binef_var = v + '_binef'\n",
    "            trans_vars.append(binef_var)\n",
    "            X_train[binef_var] = est.transform(X_train[[v]])\n",
    "            X_test[binef_var] = est.transform(X_test[[v]])\n",
    "            test[binef_var] = est.transform(test[[v]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4. Discretizing (or binning) numerical variables - Equal width discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Test the variable discretizing on a numerical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins : 10\n",
      "Wall time: 6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([array([17. , 25.1, 33.2, 41.3, 49.4, 57.5, 65.6, 73.7, 81.8, 89.9, 98. ])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Select a variable\n",
    "v = \"age\"\n",
    "\n",
    "# Binning values of a variable into new groups using equal width approach\n",
    "est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "est.fit(X_train[[v]])\n",
    "print(\"Number of bins :\", est.n_bins_[0])\n",
    "est.bin_edges_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0     658\n",
       "1.0    4292\n",
       "2.0    4839\n",
       "3.0    3109\n",
       "4.0    2279\n",
       "5.0     593\n",
       "6.0     120\n",
       "7.0      71\n",
       "8.0      34\n",
       "9.0       5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Apply the binning to a variable\n",
    "binew_v = est.transform(X_train[[v]])\n",
    "pd.DataFrame(binew_v).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Apply the variable discretizing for all numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "if enable_trans_num_ew:\n",
    "    for v in num_vars:\n",
    "        # Binning values of a variable\n",
    "        est = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
    "        est.fit(X_train[[v]])\n",
    "        # Bin the variable on train, test\n",
    "        if est.n_bins_[0] > 1:\n",
    "            binew_var = v + '_binew'\n",
    "            trans_vars.append(binew_var)\n",
    "            X_train[binew_var] = est.transform(X_train[[v]])\n",
    "            X_test[binew_var] = est.transform(X_test[[v]])\n",
    "            test[binew_var] = est.transform(test[[v]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5. Finalize value transformation\n",
    "\n",
    "- Finalize the variables list\n",
    "- Arrange the data columns\n",
    "\n",
    "<u>Note:</u> After the end of the value transformation step, we have these final lists of variables to manage:\n",
    "- num_vars\n",
    "- na_vars\n",
    "- cat_vars = cat_vars + trans_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (16000, 55)\n",
      "Validation Shape:  (4000, 55)\n",
      "Test Shape:  (10000, 55)\n"
     ]
    }
   ],
   "source": [
    "# Check Shape\n",
    "print(\"Train Shape: \", X_train.shape)\n",
    "print(\"Validation Shape: \", X_test.shape)\n",
    "print(\"Test Shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non transformed variables\n",
    "# Since all of the variables have been transformed and created new \"bins\", we dont need the originals anymore\n",
    "\n",
    "X_train = X_train.drop(num_vars, axis=1)\n",
    "X_test = X_test.drop(num_vars, axis=1)\n",
    "test = test.drop(num_vars, axis=1)\n",
    "\n",
    "X_train = X_train.drop(cat_vars, axis=1)\n",
    "X_test = X_test.drop(cat_vars, axis=1)\n",
    "test = test.drop(cat_vars, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Variable Names\n",
    "predictors = [v for v in X_train.columns if v not in id_var + target_var]\n",
    "na_vars = [v for v in X_train.columns if 'missing' in v]\n",
    "trans_vars = [v for v in predictors if v not in na_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (16000, 32)\n",
      "Validation Shape:  (4000, 32)\n",
      "Test Shape:  (10000, 32)\n"
     ]
    }
   ],
   "source": [
    "# Re-Order columns\n",
    "X_train = X_train[trans_vars + na_vars + target_var]\n",
    "X_test = X_test[trans_vars + na_vars + target_var]\n",
    "test = test[id_var + trans_vars + na_vars]\n",
    "\n",
    "# Check Shape\n",
    "print(\"Train Shape: \", X_train.shape)\n",
    "print(\"Validation Shape: \", X_test.shape)\n",
    "print(\"Test Shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Value representation (cat => num)\n",
    "\n",
    "- Categorical variable: Dummy coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1. Dummy coding\n",
    "\n",
    "<u>Note:</u> Here, we can fit the encoder on both train and test to make sure it captures all unique categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (16000, 219)\n",
      "Validation Shape:  (4000, 219)\n",
      "Test Shape:  (10000, 218)\n"
     ]
    }
   ],
   "source": [
    "# Dummy encoding trans_var\n",
    "X_train = pd.get_dummies(X_train, columns=trans_vars, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, columns=trans_vars, drop_first=True)\n",
    "test = pd.get_dummies(test, columns=trans_vars, drop_first=True)\n",
    "\n",
    "print(\"Train Shape: \", X_train.shape)\n",
    "print(\"Validation Shape: \", X_test.shape)\n",
    "print(\"Test Shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "#Reset num_vars\n",
    "num_vars = list(set(test.columns)-set(na_vars)-set(target_var) - set(id_var))\n",
    "\n",
    "#Set predictors\n",
    "predictors = num_vars + na_vars\n",
    "\n",
    "#Validate\n",
    "print(len(num_vars) + len(na_vars) + len(target_var))\n",
    "print(len(predictors) + len(target_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (b) Filter out low variance variables (or constant)\n",
    "\n",
    "During the data processing, we may accidentally create some new constant variables. Therefore, it is necessary to filter again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop # constant vars : 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Detect constant vars\n",
    "sel = VarianceThreshold(0)  # Var = 0 by default\n",
    "sel.fit(X_train[predictors])\n",
    "const_vars = [predictors[i] for i in np.where(sel.variances_ == 0)[0]]\n",
    "predictors = [v for v in predictors if v not in const_vars]\n",
    "\n",
    "# Drop from train, test\n",
    "print('Drop # constant vars :', len(const_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Drop duplicated variables\n",
    "\n",
    "Sometimes, the data processing process can create a lot of duplicated variables. In this case, it is necessary to identify and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# duplicated vars : 23\n"
     ]
    }
   ],
   "source": [
    "# Count the duplicated vars\n",
    "dup_vars = X_train[predictors].T.duplicated()\n",
    "print('# duplicated vars :', dup_vars.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop variables not in test\n",
    "# Note: we use test here to automatically ignore vars not created in the test set\n",
    "# Sometimes when we get dummies, some bins in the test are not created bc it has less data, thus we ignore the bins not in the test\n",
    "\n",
    "predictors = [v for v in test.columns if v not in id_var + target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicated vars from train, test\n",
    "predictors = [predictors[i] for i in range(0, len(predictors)) if not dup_vars[i]]\n",
    "X_train = X_train[predictors + target_var]\n",
    "X_test = X_test[predictors + target_var]\n",
    "test = test[id_var + predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (16000, 195)\n",
      "Validation Shape:  (4000, 195)\n",
      "Test Shape:  (10000, 195)\n"
     ]
    }
   ],
   "source": [
    "# Validate shapes\n",
    "print(\"Train Shape: \", X_train.shape)\n",
    "print(\"Validation Shape: \", X_test.shape)\n",
    "print(\"Test Shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Finalize the variables list\n",
    "\n",
    "<u>Note:</u> At the end of the value representation step, we have these final lists of variables to manage:\n",
    "- num_vars = num_vars + repr_vars\n",
    "- na_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:  (16000, 195)\n",
      "Validation Shape:  (4000, 195)\n",
      "Test Shape:  (10000, 195)\n"
     ]
    }
   ],
   "source": [
    "#Validate shapes\n",
    "print(\"Train Shape: \", X_train.shape)\n",
    "print(\"Validation Shape: \", X_test.shape)\n",
    "print(\"Test Shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Export the processed data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to pickle format\n",
    "X_train.to_pickle(wd +\"/data/final/train_processed.pkl\")\n",
    "X_test.to_pickle(wd + \"/data/final/validation_processed.pkl\")\n",
    "test.to_pickle(wd + \"/data/final/test_processed.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Variable selection\n",
    "\n",
    "<u>Reference:</u>  \n",
    "\n",
    "- Verbeke, W., Dejaeger, K., Martens, D., Hur, J., & Baesens, B. (2012). New insights into churn prediction in the telecommunication sector: A profit driven data mining approach. European Journal of Operational Research, 218(1), 211-229."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back the processed data\n",
    "X_train = pd.read_pickle(wd + \"/data/final/train_processed.pkl\")\n",
    "X_test = pd.read_pickle(wd + \"/data/final/validation_processed.pkl\")\n",
    "test = pd.read_pickle(wd + \"/data/final/test_processed.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall variable lists\n",
    "# Note: This is done to be able to run this second part of the notebook without running the whole pre-processing\n",
    "id_var = ['client_id']\n",
    "target_var = ['target']\n",
    "predictors = [v for v in X_train.columns if v not in id_var + target_var]\n",
    "na_vars = [v for v in X_train.columns if 'missing' in v]\n",
    "num_vars = list(set(X_train.columns)-set(na_vars)-set(target_var))\n",
    "\n",
    "# Targets\n",
    "y_train = X_train[target_var]\n",
    "y_test = X_test[target_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (a) Variable selection: Fisher Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "def FisherScore(bt, target_var, predictors):\n",
    "    \"\"\"\n",
    "    This function calculate the Fisher score of a variable.\n",
    "\n",
    "    Ref:\n",
    "    ---\n",
    "    Verbeke, W., Dejaeger, K., Martens, D., Hur, J., & Baesens, B. (2012). New insights\n",
    "    into churn prediction in the telecommunication sector: A profit driven data mining\n",
    "    approach. European Journal of Operational Research, 218(1), 211-229.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the unique values of dependent variable\n",
    "    target_var_val = bt[target_var].unique()\n",
    "    # Calculate FisherScore for each predictor\n",
    "    predictor_FisherScore = []\n",
    "    for v in predictors:\n",
    "        fs = np.abs(np.mean(bt.loc[bt[target_var]==target_var_val[0], v]) - np.mean(bt.loc[bt[target_var]==target_var_val[1], v])) / \\\n",
    "             np.sqrt(np.var(bt.loc[bt[target_var]==target_var_val[0], v]) + np.var(bt.loc[bt[target_var]==target_var_val[1], v]))\n",
    "        predictor_FisherScore.append(fs)\n",
    "    return predictor_FisherScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictor</th>\n",
       "      <th>fisherscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>pdays_bin_4</td>\n",
       "      <td>0.455002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>poutcome_remap_3</td>\n",
       "      <td>0.443917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>poutcome_remap_4</td>\n",
       "      <td>0.384148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>emp.var.rate_bin_14</td>\n",
       "      <td>0.357762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>emp.var.rate_bin_13</td>\n",
       "      <td>0.342245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              predictor  fisherscore\n",
       "70          pdays_bin_4     0.455002\n",
       "39     poutcome_remap_3     0.443917\n",
       "40     poutcome_remap_4     0.384148\n",
       "78  emp.var.rate_bin_14     0.357762\n",
       "77  emp.var.rate_bin_13     0.342245"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Calculate Fisher Score for all variable\n",
    "fs = FisherScore(X_train, target_var[0], predictors)\n",
    "fs_df = pd.DataFrame({\"predictor\":predictors, \"fisherscore\":fs})\n",
    "fs_df = fs_df.sort_values('fisherscore', ascending=False)\n",
    "fs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Visualize the Fisher Score\n",
    "plt.plot(fs_df['fisherscore'].values.squeeze())\n",
    "plt.axvline(x=20, linestyle='dashed', color='red')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(str(fs_df.shape[0]) + ' predictors')\n",
    "plt.ylabel('Fisher Score')\n",
    "plt.legend(['Fisher Score', 'Top20'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Source: \n",
    "Phan, M. (2022). Statistical & Machine Learning. [Course]. Lille: IESEG Management School. MSc in Big Data Analytics.\n",
    "\"\"\"\n",
    "# Check how AUC change when add more variables: Top n vars\n",
    "fs_scores = []\n",
    "top_n_vars = 60\n",
    "for i in range(1, top_n_vars+1):\n",
    "    if i % 100 == 0: print('Added # top vars :', i)\n",
    "    top_n_predictors = fs_df['predictor'][:i]\n",
    "    clf = LogisticRegression()\n",
    "    fs_scores.append(cross_validate(clf, X_train[top_n_predictors], X_train[target_var].values.squeeze(),\n",
    "                                    scoring='roc_auc', cv=5, verbose=0, n_jobs=-1, return_train_score=True))\n",
    "\n",
    "# How the AUC curve looks like when adding top vars\n",
    "plt.plot([s['train_score'].mean() for s in fs_scores], color='blue')\n",
    "plt.plot([s['test_score'].mean() for s in fs_scores], color='red')\n",
    "plt.xlabel('# vars')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected # vars : 55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['pdays_bin_4', 'poutcome_remap_3', 'poutcome_remap_4',\n",
       "       'emp.var.rate_bin_14', 'emp.var.rate_bin_13', 'contact_remap_3',\n",
       "       'cons.price.idx_bin_21', 'cons.conf.idx_bin_20',\n",
       "       'nr.employed_bin_8', 'previous_bin_4', 'emp.var.rate_bin_5',\n",
       "       'nr.employed_bin_9', 'month_remap_12', 'cons.price.idx_bin_5',\n",
       "       'emp.var.rate_bin_8', 'cons.conf.idx_bin_23', 'nr.employed_bin_6',\n",
       "       'cons.conf.idx_bin_17', 'nr.employed_bin_7', 'previous_bin_3',\n",
       "       'age_bin_50', 'default_remap_2', 'cons.conf.idx_bin_18',\n",
       "       'cons.price.idx_bin_4', 'cons.conf.idx_bin_27', 'month_remap_10',\n",
       "       'euribor3m_bin_8', 'euribor3m_bin_4', 'cons.price.idx_bin_19',\n",
       "       'euribor3m_bin_6', 'month_remap_15', 'cons.conf.idx_bin_21',\n",
       "       'cons.conf.idx_bin_13', 'cons.conf.idx_bin_15', 'euribor3m_bin_14',\n",
       "       'cons.price.idx_bin_11', 'job_remap_4', 'euribor3m_bin_62',\n",
       "       'cons.conf.idx_bin_11', 'cons.price.idx_bin_20', 'month_remap_16',\n",
       "       'cons.price.idx_bin_24', 'cons.conf.idx_bin_25',\n",
       "       'euribor3m_bin_23', 'euribor3m_bin_12', 'job_remap_17',\n",
       "       'euribor3m_bin_15', 'job_remap_11', 'emp.var.rate_bin_6',\n",
       "       'cons.conf.idx_bin_28', 'euribor3m_bin_58', 'euribor3m_bin_11',\n",
       "       'euribor3m_bin_39', 'cons.conf.idx_bin_12', 'euribor3m_bin_84'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the top variables based on Fisher Score\n",
    "n_top_fs_vars = 55  # Top FS vars\n",
    "top_fs_vars = fs_df['predictor'].values[:n_top_fs_vars]\n",
    "print(\"Selected # vars :\", len(top_fs_vars))\n",
    "top_fs_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set here Variables to use in modeling\n",
    "# we can choose whether we want to use fisher score variables or all the predictors. \n",
    "used_vars = top_fs_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(model, printname):\n",
    "    \"\"\"\n",
    "    This function returns the performance of AUC and Accuracy for the specified model.\n",
    "    \"model\" represents the name of the model function, and \"printname\" is a string to give the model a name in the overview table. \n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #Train\n",
    "    result = {}\n",
    "    predictions   = model.predict(X_train[used_vars])\n",
    "    probabilities = pd.DataFrame(model.predict_proba(X_train[used_vars]))[1]\n",
    "    accuracy      = accuracy_score(y_train, predictions)\n",
    "    auc           = roc_auc_score(np.array(y_train),np.array(probabilities))\n",
    "    result[str(printname)] = {\"Train_Accuracy\":accuracy, \"Train_AUC\":auc}\n",
    "    train_perf = pd.DataFrame(result).transpose()\n",
    "\n",
    "    #Test\n",
    "    result = {}\n",
    "    predictions   = model.predict(X_test[used_vars])\n",
    "    probabilities = pd.DataFrame(model.predict_proba(X_test[used_vars]))[1]\n",
    "    accuracy      = accuracy_score(y_test, predictions)\n",
    "    auc           = roc_auc_score(np.array(y_test),np.array(probabilities))\n",
    "    result[str(printname)] = {\"Test_Accuracy\":accuracy,\"Test_AUC\":auc}\n",
    "    test_perf = pd.DataFrame(result).transpose()\n",
    "\n",
    "    #Compare Train and Test AUC\n",
    "    perf_total = train_perf.merge(test_perf, left_index=True, right_index=True)\n",
    "    return(perf_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Simple Models (Without Gridsearch and Cross-Validation)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Logistic Regression\n",
    "np.random.seed(1)\n",
    "lr=LogisticRegression().fit(X_train[used_vars], y_train)\n",
    "lr_perf = performance(lr, 'Logistic Regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Forest\n",
    "np.random.seed(1)\n",
    "rf=RandomForestClassifier().fit(X_train[used_vars], y_train.values.ravel())\n",
    "rf_perf = performance(rf, 'Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Gradient Boosting\n",
    "np.random.seed(1)\n",
    "xgbc =GradientBoostingClassifier().fit(X_train[used_vars], y_train)\n",
    "xgbc_perf = performance(xgbc, 'Gradient Boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KNN\n",
    "np.random.seed(1)\n",
    "knn =KNeighborsClassifier().fit(X_train[used_vars], y_train)\n",
    "knn_perf = performance(knn, 'KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit SVM\n",
    "np.random.seed(1)\n",
    "svm =SVC(probability=True).fit(X_train[used_vars], y_train)\n",
    "svm_perf = performance(svm, 'SVM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Test_AUC</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.793496</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>0.801217</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.799968</td>\n",
       "      <td>0.904500</td>\n",
       "      <td>0.801207</td>\n",
       "      <td>0.90125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.838477</td>\n",
       "      <td>0.919687</td>\n",
       "      <td>0.783546</td>\n",
       "      <td>0.89525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.779126</td>\n",
       "      <td>0.905312</td>\n",
       "      <td>0.750977</td>\n",
       "      <td>0.89525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.705586</td>\n",
       "      <td>0.905250</td>\n",
       "      <td>0.681501</td>\n",
       "      <td>0.90225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Train_AUC  Train_Accuracy  Test_AUC  Test_Accuracy\n",
       "Logistic Regression   0.793496        0.900438  0.801217        0.90025\n",
       "Gradient Boosting     0.799968        0.904500  0.801207        0.90125\n",
       "Random Forest         0.838477        0.919687  0.783546        0.89525\n",
       "KNN                   0.779126        0.905312  0.750977        0.89525\n",
       "SVM                   0.705586        0.905250  0.681501        0.90225"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full performance\n",
    "perf_overview = pd.concat([xgbc_perf, rf_perf, lr_perf, knn_perf, svm_perf]).sort_values(by=\"Test_AUC\", ascending=False)\n",
    "perf_overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of Logistic Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = lr.predict(X_test[used_vars])\n",
    "probabilities = pd.DataFrame(lr.predict_proba(X_test[used_vars]))[1]\n",
    "auc         = roc_auc_score(np.array(y_test),np.array(probabilities))\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'AUC Score: {0}'.format(np.round(auc,4))\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "False Positive Rate=%{x}<br>True Positive Rate=%{y}<extra></extra>",
         "legendgroup": "",
         "line": {
          "color": "#636efa"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "stackgroup": "1",
         "type": "scatter",
         "x": [
          0,
          0,
          0,
          0.0002820078962210942,
          0.0002820078962210942,
          0.0005640157924421884,
          0.0005640157924421884,
          0.0008460236886632825,
          0.0008460236886632825,
          0.0011280315848843769,
          0.001692047377326565,
          0.001692047377326565,
          0.001692047377326565,
          0.001692047377326565,
          0.0019740552735476595,
          0.0019740552735476595,
          0.0019740552735476595,
          0.0019740552735476595,
          0.0019740552735476595,
          0.0028200789622109417,
          0.003102086858432036,
          0.003102086858432036,
          0.00338409475465313,
          0.00338409475465313,
          0.0036661026508742244,
          0.0036661026508742244,
          0.003948110547095319,
          0.003948110547095319,
          0.004230118443316413,
          0.0045121263395375075,
          0.0045121263395375075,
          0.0045121263395375075,
          0.004794134235758601,
          0.004794134235758601,
          0.005076142131979695,
          0.005076142131979695,
          0.00535815002820079,
          0.00535815002820079,
          0.0056401579244218835,
          0.0056401579244218835,
          0.0064861816130851666,
          0.0064861816130851666,
          0.007050197405527355,
          0.007050197405527355,
          0.007332205301748449,
          0.007332205301748449,
          0.007614213197969543,
          0.007614213197969543,
          0.007614213197969543,
          0.007614213197969543,
          0.007614213197969543,
          0.007614213197969543,
          0.008460236886632826,
          0.009024252679075015,
          0.009306260575296108,
          0.009306260575296108,
          0.009588268471517203,
          0.009870276367738297,
          0.009870276367738297,
          0.010998307952622674,
          0.011844331641285956,
          0.012126339537507051,
          0.012408347433728144,
          0.012408347433728144,
          0.012690355329949238,
          0.012972363226170333,
          0.01353637901861252,
          0.01353637901861252,
          0.01353637901861252,
          0.01353637901861252,
          0.01410039481105471,
          0.01410039481105471,
          0.014382402707275803,
          0.014382402707275803,
          0.014664410603496898,
          0.014664410603496898,
          0.014946418499717992,
          0.014946418499717992,
          0.01551043429216018,
          0.015792442188381276,
          0.016638465877044557,
          0.01692047377326565,
          0.017202481669486746,
          0.01748448956570784,
          0.01748448956570784,
          0.01748448956570784,
          0.018612521150592216,
          0.018612521150592216,
          0.01889452904681331,
          0.01889452904681331,
          0.01889452904681331,
          0.01889452904681331,
          0.0194585448392555,
          0.0194585448392555,
          0.019740552735476594,
          0.02086858432036097,
          0.02086858432036097,
          0.021150592216582064,
          0.021714608009024253,
          0.021996615905245348,
          0.022278623801466443,
          0.022278623801466443,
          0.02284263959390863,
          0.023124647490129723,
          0.023124647490129723,
          0.023688663282571912,
          0.024252679075014102,
          0.024816694867456288,
          0.024816694867456288,
          0.025098702763677382,
          0.025380710659898477,
          0.02566271855611957,
          0.028764805414551606,
          0.0290468133107727,
          0.03017484489565708,
          0.031302876480541454,
          0.03186689227298364,
          0.03186689227298364,
          0.03327693175408911,
          0.03327693175408911,
          0.034122955442752394,
          0.03440496333897349,
          0.03496897913141568,
          0.03553299492385787,
          0.03553299492385787,
          0.03553299492385787,
          0.03609701071630006,
          0.03637901861252115,
          0.03666102650874224,
          0.03666102650874224,
          0.03722504230118443,
          0.03778905809362662,
          0.03778905809362662,
          0.03807106598984772,
          0.03976311336717428,
          0.04060913705583756,
          0.04060913705583756,
          0.04117315284827975,
          0.04145516074450085,
          0.044557247602932885,
          0.04483925549915398,
          0.045403271291596166,
          0.045967287084038355,
          0.04709531866892273,
          0.047659334461364916,
          0.047941342357586014,
          0.048505358150028204,
          0.050197405527354765,
          0.050197405527354765,
          0.05047941342357586,
          0.05104342921601805,
          0.05273547659334461,
          0.05386350817822899,
          0.05386350817822899,
          0.05442752397067118,
          0.05442752397067118,
          0.05470953186689227,
          0.05470953186689227,
          0.05527354765933446,
          0.05527354765933446,
          0.05555555555555555,
          0.05555555555555555,
          0.05583756345177665,
          0.05583756345177665,
          0.05752961082910321,
          0.05865764241398759,
          0.05950366610265088,
          0.06006768189509306,
          0.06091370558375635,
          0.06175972927241963,
          0.06175972927241963,
          0.062887760857304,
          0.0631697687535251,
          0.06457980823463057,
          0.06542583192329385,
          0.06542583192329385,
          0.06655386350817823,
          0.06711787930062042,
          0.06965595036661026,
          0.06965595036661026,
          0.06993795826283136,
          0.06993795826283136,
          0.0724760293288212,
          0.0724760293288212,
          0.0730400451212634,
          0.0730400451212634,
          0.07332205301748448,
          0.07388606880992668,
          0.07416807670614778,
          0.07473209249858996,
          0.07501410039481106,
          0.07529610829103214,
          0.07529610829103214,
          0.07557811618725324,
          0.07586012408347434,
          0.07586012408347434,
          0.07642413987591652,
          0.07642413987591652,
          0.07670614777213762,
          0.07670614777213762,
          0.07698815566835872,
          0.07698815566835872,
          0.07839819514946418,
          0.07896221094190638,
          0.07896221094190638,
          0.07952622673434856,
          0.08009024252679076,
          0.08065425831923294,
          0.0823463056965595,
          0.0829103214890017,
          0.08403835307388607,
          0.08403835307388607,
          0.08516638465877044,
          0.08516638465877044,
          0.08601240834743373,
          0.08657642413987592,
          0.08685843203609701,
          0.0874224478285392,
          0.0874224478285392,
          0.08855047941342357,
          0.08967851099830795,
          0.08967851099830795,
          0.09080654258319233,
          0.09165256627185561,
          0.09249858996051889,
          0.09306260575296109,
          0.09362662154540327,
          0.09419063733784545,
          0.09531866892272983,
          0.09616469261139311,
          0.09757473209249859,
          0.11816130851663846,
          0.11872532430908066,
          0.11900733220530176,
          0.11928934010152284,
          0.11928934010152284,
          0.12013536379018612,
          0.12380146644106035,
          0.12436548223350254,
          0.12436548223350254,
          0.12464749012972363,
          0.12549351381838691,
          0.1260575296108291,
          0.1305696559503666,
          0.1305696559503666,
          0.1311336717428088,
          0.1319796954314721,
          0.1322617033276932,
          0.13395375070501975,
          0.13479977439368301,
          0.1350817822899041,
          0.1359278059785674,
          0.13649182177100957,
          0.14156796390298929,
          0.14184997179921038,
          0.14241398759165258,
          0.14269599548787365,
          0.1449520586576424,
          0.1457980823463057,
          0.1460800902425268,
          0.18668922729836435,
          0.18668922729836435,
          0.18697123519458544,
          0.18781725888324874,
          0.18809926677946984,
          0.19120135363790186,
          0.19176536943034406,
          0.19176536943034406,
          0.19232938522278623,
          0.2098138747884941,
          0.21009588268471519,
          0.21122391426959955,
          0.21206993795826284,
          0.2131979695431472,
          0.21912013536379019,
          0.22109419063733785,
          0.2222222222222222,
          0.2225042301184433,
          0.2227862380146644,
          0.22419627749576987,
          0.22476029328821207,
          0.28849407783417935,
          0.2893401015228426,
          0.2899041173152848,
          0.29018612521150594,
          0.2966723068245911,
          0.2969543147208122,
          0.29723632261703326,
          0.29723632261703326,
          0.2980823463056966,
          0.2992103778905809,
          0.32205301748448956,
          0.3386914833615341,
          0.3386914833615341,
          0.3437676254935138,
          0.344331641285956,
          0.3917089678510998,
          0.39199097574732095,
          0.3925549915397631,
          0.3928369994359842,
          0.3928369994359842,
          0.3934010152284264,
          0.3939650310208686,
          0.4125775521714608,
          0.413141567963903,
          0.41398759165256627,
          0.4165256627185561,
          0.42160180485053583,
          0.42216582064297803,
          0.4230118443316413,
          0.42442188381274676,
          0.42526790750141,
          0.4413423575860124,
          0.4509306260575296,
          0.5180485053581501,
          0.5186125211505922,
          0.5197405527354766,
          0.5236886632825719,
          0.5329949238578681,
          0.5332769317540891,
          0.5341229554427523,
          0.5346869712351946,
          0.5360970107163,
          0.5372250423011844,
          0.5375070501974055,
          0.5428652002256064,
          0.5437112239142696,
          0.5439932318104906,
          0.5916525662718556,
          0.5916525662718556,
          0.5924985899605189,
          0.59278059785674,
          0.5939086294416244,
          0.5950366610265088,
          0.6003948110547095,
          0.6009588268471517,
          0.6065989847715736,
          0.6068809926677947,
          0.6088550479413424,
          0.6094190637337845,
          0.622673434856176,
          0.6246474901297236,
          0.6252115059221658,
          0.625775521714608,
          0.6269035532994924,
          0.6300056401579244,
          0.6322617033276932,
          0.6328257191201354,
          0.6489001692047377,
          0.64946418499718,
          0.6500282007896221,
          0.6717428087986463,
          0.6734348561759729,
          0.6742808798646363,
          0.6751269035532995,
          0.6765369430344049,
          0.6790750141003948,
          0.682741116751269,
          0.6830231246474902,
          0.7019176536943035,
          0.7202481669486746,
          0.7306824591088551,
          0.7315284827975184,
          0.7366046249294981,
          0.7368866328257191,
          0.7374506486181613,
          0.7408347433728144,
          0.7442188381274676,
          0.7450648618161309,
          0.7467569091934574,
          0.7473209249858996,
          0.7476029328821207,
          0.7509870276367738,
          0.7521150592216582,
          0.7523970671178793,
          0.7526790750141004,
          0.7535250987027636,
          0.7549351381838691,
          0.7698815566835872,
          0.7721376198533559,
          0.772419627749577,
          0.7729836435420192,
          0.7769317540891145,
          0.7772137619853355,
          0.7777777777777778,
          0.7791878172588832,
          0.7794698251551043,
          0.7803158488437676,
          0.7805978567399887,
          0.7839819514946419,
          0.7842639593908629,
          0.7884940778341794,
          0.7910321489001692,
          0.7927241962774958,
          0.8423575860124084,
          0.8426395939086294,
          0.8432036097010717,
          0.8434856175972927,
          0.8446136491821771,
          0.8767625493513819,
          0.8784545967287084,
          0.8795826283135928,
          0.8804286520022561,
          0.8809926677946982,
          0.8815566835871405,
          0.8979131415679639,
          0.9086294416243654,
          0.9089114495205866,
          0.9103214890016921,
          0.9108855047941342,
          0.9297800338409475,
          0.9303440496333897,
          0.9469825155104343,
          0.9475465313028765,
          0.9483925549915397,
          0.949238578680203,
          0.9627749576988156,
          0.9717992103778906,
          0.9720812182741116,
          0.9844895657078399,
          0.9853355893965031,
          0.9892836999435984,
          0.9963338973491258,
          1
         ],
         "xaxis": "x",
         "y": [
          0,
          0.0022026431718061676,
          0.004405286343612335,
          0.004405286343612335,
          0.00881057268722467,
          0.00881057268722467,
          0.013215859030837005,
          0.015418502202643172,
          0.022026431718061675,
          0.022026431718061675,
          0.024229074889867842,
          0.02643171806167401,
          0.03524229074889868,
          0.039647577092511016,
          0.039647577092511016,
          0.05726872246696035,
          0.06387665198237885,
          0.08149779735682819,
          0.09030837004405286,
          0.09030837004405286,
          0.09251101321585903,
          0.0947136563876652,
          0.0947136563876652,
          0.09691629955947137,
          0.09911894273127753,
          0.10352422907488987,
          0.11013215859030837,
          0.11233480176211454,
          0.11233480176211454,
          0.11894273127753303,
          0.12555066079295155,
          0.1299559471365639,
          0.13215859030837004,
          0.14096916299559473,
          0.14096916299559473,
          0.14977973568281938,
          0.14977973568281938,
          0.15198237885462554,
          0.15198237885462554,
          0.15638766519823788,
          0.15638766519823788,
          0.15859030837004406,
          0.16079295154185022,
          0.16299559471365638,
          0.16519823788546256,
          0.16740088105726872,
          0.16740088105726872,
          0.17180616740088106,
          0.1762114537444934,
          0.17841409691629956,
          0.1828193832599119,
          0.18502202643171806,
          0.1894273127753304,
          0.19162995594713655,
          0.19162995594713655,
          0.1960352422907489,
          0.19823788546255505,
          0.19823788546255505,
          0.20484581497797358,
          0.21145374449339208,
          0.21145374449339208,
          0.21365638766519823,
          0.21365638766519823,
          0.21585903083700442,
          0.21585903083700442,
          0.21806167400881057,
          0.21806167400881057,
          0.22026431718061673,
          0.22466960352422907,
          0.22687224669603523,
          0.22687224669603523,
          0.2290748898678414,
          0.23348017621145375,
          0.2422907488986784,
          0.2422907488986784,
          0.24669603524229075,
          0.24669603524229075,
          0.2488986784140969,
          0.2533039647577093,
          0.2533039647577093,
          0.2533039647577093,
          0.2533039647577093,
          0.2643171806167401,
          0.2643171806167401,
          0.2665198237885463,
          0.27312775330396477,
          0.2775330396475771,
          0.28193832599118945,
          0.28193832599118945,
          0.2841409691629956,
          0.28854625550660795,
          0.2907488986784141,
          0.29515418502202645,
          0.2973568281938326,
          0.2973568281938326,
          0.2973568281938326,
          0.29955947136563876,
          0.30176211453744495,
          0.30616740088105726,
          0.30837004405286345,
          0.30837004405286345,
          0.31057268722466963,
          0.31057268722466963,
          0.31057268722466963,
          0.31277533039647576,
          0.31277533039647576,
          0.31277533039647576,
          0.31938325991189426,
          0.32158590308370044,
          0.32158590308370044,
          0.32599118942731276,
          0.32819383259911894,
          0.3392070484581498,
          0.3392070484581498,
          0.3502202643171806,
          0.3502202643171806,
          0.3502202643171806,
          0.3524229074889868,
          0.3524229074889868,
          0.3568281938325991,
          0.36784140969163,
          0.36784140969163,
          0.3700440528634361,
          0.3700440528634361,
          0.3744493392070485,
          0.3766519823788546,
          0.3766519823788546,
          0.3788546255506608,
          0.3788546255506608,
          0.3832599118942731,
          0.3832599118942731,
          0.3832599118942731,
          0.3854625550660793,
          0.3854625550660793,
          0.3964757709251101,
          0.3964757709251101,
          0.3986784140969163,
          0.3986784140969163,
          0.4008810572687225,
          0.40969162995594716,
          0.40969162995594716,
          0.41409691629955947,
          0.41409691629955947,
          0.41409691629955947,
          0.41629955947136565,
          0.41629955947136565,
          0.41629955947136565,
          0.41629955947136565,
          0.42070484581497797,
          0.42070484581497797,
          0.42290748898678415,
          0.42290748898678415,
          0.4251101321585903,
          0.43171806167400884,
          0.43171806167400884,
          0.43392070484581496,
          0.43392070484581496,
          0.43612334801762115,
          0.43832599118942733,
          0.44273127753303965,
          0.44273127753303965,
          0.44493392070484583,
          0.44493392070484583,
          0.44713656387665196,
          0.45154185022026433,
          0.45154185022026433,
          0.45154185022026433,
          0.45594713656387664,
          0.45594713656387664,
          0.4581497797356828,
          0.460352422907489,
          0.460352422907489,
          0.4647577092511013,
          0.4713656387665198,
          0.4713656387665198,
          0.473568281938326,
          0.473568281938326,
          0.473568281938326,
          0.48237885462555063,
          0.4845814977973568,
          0.4845814977973568,
          0.486784140969163,
          0.4911894273127753,
          0.4933920704845815,
          0.4933920704845815,
          0.4955947136563877,
          0.4955947136563877,
          0.4955947136563877,
          0.5,
          0.5022026431718062,
          0.5022026431718062,
          0.5044052863436124,
          0.5066079295154186,
          0.5088105726872246,
          0.5088105726872246,
          0.513215859030837,
          0.513215859030837,
          0.5154185022026432,
          0.5154185022026432,
          0.5176211453744494,
          0.5176211453744494,
          0.5198237885462555,
          0.5198237885462555,
          0.5198237885462555,
          0.5220264317180616,
          0.5220264317180616,
          0.5220264317180616,
          0.5220264317180616,
          0.5352422907488987,
          0.5374449339207048,
          0.5374449339207048,
          0.539647577092511,
          0.539647577092511,
          0.5418502202643172,
          0.5418502202643172,
          0.5462555066079295,
          0.5462555066079295,
          0.5462555066079295,
          0.5506607929515418,
          0.552863436123348,
          0.552863436123348,
          0.5550660792951542,
          0.5550660792951542,
          0.5550660792951542,
          0.5572687224669604,
          0.5572687224669604,
          0.5572687224669604,
          0.5638766519823789,
          0.5638766519823789,
          0.5682819383259912,
          0.5682819383259912,
          0.6079295154185022,
          0.6079295154185022,
          0.6101321585903083,
          0.6101321585903083,
          0.6123348017621145,
          0.6123348017621145,
          0.6167400881057269,
          0.6167400881057269,
          0.6189427312775331,
          0.6211453744493393,
          0.6233480176211453,
          0.6233480176211453,
          0.6255506607929515,
          0.6277533039647577,
          0.6277533039647577,
          0.6277533039647577,
          0.6277533039647577,
          0.6277533039647577,
          0.6277533039647577,
          0.6299559471365639,
          0.6299559471365639,
          0.6299559471365639,
          0.6321585903083701,
          0.6321585903083701,
          0.6321585903083701,
          0.6321585903083701,
          0.6409691629955947,
          0.6409691629955947,
          0.6409691629955947,
          0.6784140969162996,
          0.6806167400881057,
          0.6806167400881057,
          0.6828193832599119,
          0.6828193832599119,
          0.6828193832599119,
          0.6828193832599119,
          0.6850220264317181,
          0.6850220264317181,
          0.7026431718061674,
          0.7026431718061674,
          0.7026431718061674,
          0.7026431718061674,
          0.7026431718061674,
          0.7026431718061674,
          0.7026431718061674,
          0.7026431718061674,
          0.7026431718061674,
          0.7048458149779736,
          0.7048458149779736,
          0.7048458149779736,
          0.7290748898678414,
          0.7290748898678414,
          0.7290748898678414,
          0.7290748898678414,
          0.73568281938326,
          0.73568281938326,
          0.7400881057268722,
          0.7422907488986784,
          0.7444933920704846,
          0.7444933920704846,
          0.7555066079295154,
          0.7687224669603524,
          0.7709251101321586,
          0.7709251101321586,
          0.7709251101321586,
          0.7973568281938326,
          0.7973568281938326,
          0.7973568281938326,
          0.7973568281938326,
          0.7995594713656388,
          0.7995594713656388,
          0.7995594713656388,
          0.8083700440528634,
          0.8083700440528634,
          0.8083700440528634,
          0.8083700440528634,
          0.8105726872246696,
          0.8105726872246696,
          0.8105726872246696,
          0.8105726872246696,
          0.8105726872246696,
          0.8215859030837004,
          0.8259911894273128,
          0.8634361233480177,
          0.8634361233480177,
          0.8656387665198237,
          0.8656387665198237,
          0.8700440528634361,
          0.8700440528634361,
          0.8700440528634361,
          0.8700440528634361,
          0.8700440528634361,
          0.8700440528634361,
          0.8700440528634361,
          0.8700440528634361,
          0.8700440528634361,
          0.8700440528634361,
          0.8810572687224669,
          0.8832599118942731,
          0.8832599118942731,
          0.8832599118942731,
          0.8832599118942731,
          0.8832599118942731,
          0.8832599118942731,
          0.8832599118942731,
          0.8854625550660793,
          0.8854625550660793,
          0.8876651982378855,
          0.8876651982378855,
          0.8876651982378855,
          0.8898678414096917,
          0.8898678414096917,
          0.8898678414096917,
          0.8898678414096917,
          0.8964757709251101,
          0.8986784140969163,
          0.8986784140969163,
          0.9118942731277533,
          0.9118942731277533,
          0.9118942731277533,
          0.9185022026431718,
          0.9185022026431718,
          0.920704845814978,
          0.920704845814978,
          0.920704845814978,
          0.920704845814978,
          0.9229074889867841,
          0.9229074889867841,
          0.9251101321585903,
          0.9273127753303965,
          0.9273127753303965,
          0.9273127753303965,
          0.9317180616740088,
          0.9317180616740088,
          0.9317180616740088,
          0.9317180616740088,
          0.9361233480176211,
          0.9361233480176211,
          0.9361233480176211,
          0.9361233480176211,
          0.9361233480176211,
          0.9361233480176211,
          0.9361233480176211,
          0.9383259911894273,
          0.9383259911894273,
          0.9383259911894273,
          0.9383259911894273,
          0.9449339207048458,
          0.9449339207048458,
          0.9449339207048458,
          0.9449339207048458,
          0.9449339207048458,
          0.9449339207048458,
          0.9449339207048458,
          0.947136563876652,
          0.947136563876652,
          0.947136563876652,
          0.947136563876652,
          0.947136563876652,
          0.947136563876652,
          0.947136563876652,
          0.947136563876652,
          0.947136563876652,
          0.960352422907489,
          0.960352422907489,
          0.960352422907489,
          0.960352422907489,
          0.960352422907489,
          0.9647577092511013,
          0.9647577092511013,
          0.9647577092511013,
          0.9647577092511013,
          0.9647577092511013,
          0.9647577092511013,
          0.9669603524229075,
          0.9669603524229075,
          0.9669603524229075,
          0.9669603524229075,
          0.9669603524229075,
          0.9801762114537445,
          0.9801762114537445,
          0.9823788546255506,
          0.9845814977973568,
          0.9845814977973568,
          0.9845814977973568,
          0.9845814977973568,
          0.9889867841409692,
          0.9889867841409692,
          0.9955947136563876,
          0.9955947136563876,
          0.9977973568281938,
          0.9977973568281938,
          1
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 500,
        "legend": {
         "tracegroupgap": 0
        },
        "shapes": [
         {
          "line": {
           "dash": "dash"
          },
          "type": "line",
          "x0": 0,
          "x1": 1,
          "y0": 0,
          "y1": 1
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "ROC Curve (AUC=0.8012)"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "False Positive Rate"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "x",
         "scaleratio": 1,
         "title": {
          "text": "True Positive Rate"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_score = lr.predict_proba(X_test[used_vars])[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "fig = px.area(\n",
    "    x=fpr, y=tpr,\n",
    "    title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n",
    "    labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "    width=700, height=500\n",
    ")\n",
    "fig.add_shape(\n",
    "    type='line', line=dict(dash='dash'),\n",
    "    x0=0, x1=1, y0=0, y1=1\n",
    ")\n",
    "\n",
    "fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "fig.update_xaxes(constrain='domain')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Hyperparameter Tuning\n",
    "We choose to perform Hyperparams on Logistic, Gradient Boosting, and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1)\n",
    "\n",
    "#Set a Parameter Space\n",
    "# space={\"C\":np.logspace(-4, 4, 50),\n",
    "#     \"penalty\":[\"l1\", \"l2\"]} # l1 lasso l2 ridge\n",
    "\n",
    "#Grid Search\n",
    "#gs=GridSearchCV(LogisticRegression(), space, cv=10)\n",
    "\n",
    "# Fit Model\n",
    "#lr_gs = gs.fit(X_train[used_vars],y_train)\n",
    "\n",
    "#Show performance\n",
    "#print('Config: %s' % lr_gs.best_params_)\n",
    "#lr_gs_perf = performance(lr_gs, 'Logistic Regression GS')\n",
    "#lr_gs_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Logistic Regression\n",
    "np.random.seed(1)\n",
    "lr_gs=LogisticRegression(C = 0.019306977288832496, penalty = 'l2').fit(X_train[used_vars], y_train)\n",
    "lr_gs_perf = performance(lr_gs, 'Logistic Regression GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic 2\n",
    "np.random.seed(1)\n",
    "lr_gs2=LogisticRegression(C= 0.2442053094548655, penalty= 'l2', solver = 'lbfgs').fit(X_train[used_vars], y_train)\n",
    "lr_perf_gs2 = performance(lr_gs2, 'Logistic Regression GS2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic 3\n",
    "np.random.seed(1)\n",
    "lr_gs3=LogisticRegression(C= 0.09651957137350628, penalty= 'l2', solver = 'lbfgs').fit(X_train[used_vars], y_train)\n",
    "lr_perf_gs3 = performance(lr_gs3, 'Logistic Regression GS3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Nour 1\n",
    "np.random.seed(1)\n",
    "lr_nour1=LogisticRegression(C= 46.41588833612777, penalty= 'l2', solver = 'lbfgs').fit(X_train[used_vars], y_train)\n",
    "lr_perf_nour1 = performance(lr_nour1, 'Logistic Regression N1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Nour 2\n",
    "np.random.seed(1)\n",
    "lr_nour2=LogisticRegression(C= 40960000.0, penalty= 'l2', solver = 'lbfgs').fit(X_train[used_vars], y_train)\n",
    "lr_perf_nour2 = performance(lr_nour2, 'Logistic Regression N2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Nour 3\n",
    "np.random.seed(1)\n",
    "lr_nour3=LogisticRegression(C= 62.505519252739695, penalty= 'l2', solver = 'lbfgs').fit(X_train[used_vars], y_train)\n",
    "lr_perf_nour3 = performance(lr_nour3, 'Logistic Regression N3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Nour 4\n",
    "np.random.seed(1)\n",
    "lr_nour4=LogisticRegression(C=  1.2589254117941673, penalty= 'l2', solver = 'newton-cg').fit(X_train[used_vars], y_train)\n",
    "lr_perf_nour4 = performance(lr_nour4, 'Logistic Regression N4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Nour 5\n",
    "np.random.seed(1)\n",
    "lr_nour5 = LogisticRegression(C=  11.513953993264476, penalty= 'l2', solver = 'lbfgs').fit(X_train[used_vars], y_train)\n",
    "lr_perf_nour5 = performance(lr_nour5, 'Logistic Regression N5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Nour 6\n",
    "np.random.seed(1)\n",
    "lr_nour6=LogisticRegression(C=  1e+20, penalty= 'l2', solver = 'lbfgs').fit(X_train[used_vars], y_train)\n",
    "lr_perf_nour6 = performance(lr_nour6, 'Logistic Regression N6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Nour 7\n",
    "np.random.seed(1)\n",
    "lr_nour7=LogisticRegression(C=  640884341.8931684, penalty= 'l2', solver = 'lbfgs').fit(X_train[used_vars], y_train)\n",
    "lr_perf_nour7 = performance(lr_nour7, 'Logistic Regression N7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Nour 8\n",
    "np.random.seed(1)\n",
    "lr_nour8=LogisticRegression(C=  2.3741612048163567, penalty= 'l1', solver = 'liblinear').fit(X_train[used_vars], y_train)\n",
    "lr_perf_nour8 = performance(lr_nour8, 'Logistic Regression N8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Nour 9\n",
    "np.random.seed(1)\n",
    "lr_nour9=LogisticRegression(C=  1.2589254117941673, penalty= 'l1', solver = 'liblinear').fit(X_train[used_vars], y_train)\n",
    "lr_perf_nour9 = performance(lr_nour9, 'Logistic Regression N9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// Parameter Space taken from:\n",
    "#// https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# # Number of trees in random forest\n",
    "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 100)]\n",
    "# # Number of features to consider at every split\n",
    "# max_features = ['auto']\n",
    "# # Maximum number of levels in tree\n",
    "# max_depth = [int(x) for x in np.linspace(5, 50, num = 1)]\n",
    "# max_depth.append(None)\n",
    "# # Minimum number of samples required to split a node\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# # Minimum number of samples required at each leaf node\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# # Method of selecting samples for training each tree\n",
    "# bootstrap = [True]\n",
    "\n",
    "# # Create the random space grid\n",
    "# space = {'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap}\n",
    "\n",
    "#Grid Search\n",
    "#rf_gs = RandomizedSearchCV(RandomForestClassifier(), space, cv=5)\n",
    "#results = rf_gs.fit(X_train[used_vars],y_train.values.ravel())\n",
    "\n",
    "# report performance\n",
    "#print('Config: %s' % results.best_params_)\n",
    "#rf_gs_perf = performance(rf_gs, 'Random Forest GS')\n",
    "#rf_gs_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest 1\n",
    "np.random.seed(1)\n",
    "rf_gs=RandomForestClassifier(n_estimators = 579, \n",
    "                        min_samples_split = 10, \n",
    "                        min_samples_leaf = 4, \n",
    "                        max_features = 'auto', \n",
    "                        max_depth = 5, \n",
    "                        bootstrap = True).fit(X_train[used_vars], y_train.values.ravel())\n",
    "\n",
    "# Show performance\n",
    "rf_gs_perf = performance(rf_gs, 'Random Forest GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest 2\n",
    "np.random.seed(1)\n",
    "rf_gs2=RandomForestClassifier(n_estimators = 500, \n",
    "                        criterion = 'entropy',\n",
    "                        max_features = 'sqrt', \n",
    "                        max_depth = 5, \n",
    "                        bootstrap = True).fit(X_train[used_vars], y_train.values.ravel())\n",
    "\n",
    "# Show performance\n",
    "rf_gs2_perf = performance(rf_gs2, 'Random Forest GS2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest 3\n",
    "np.random.seed(1)\n",
    "rf_gs3=RandomForestClassifier(n_estimators = 1000, \n",
    "                        criterion = 'entropy',\n",
    "                        max_features = 'auto', \n",
    "                        max_depth = 5).fit(X_train[used_vars], y_train.values.ravel())\n",
    "\n",
    "# Show performance\n",
    "rf_gs3_perf = performance(rf_gs3, 'Random Forest GS3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest 4\n",
    "np.random.seed(1)\n",
    "rf_gs4=RandomForestClassifier(n_estimators = 2000, \n",
    "                        criterion = 'entropy',\n",
    "                        max_features = 'auto', \n",
    "                        max_depth = 5, \n",
    "                        bootstrap = True).fit(X_train[used_vars], y_train.values.ravel())\n",
    "\n",
    "# Show performance\n",
    "rf_gs4_perf = performance(rf_gs4, 'Random Forest GS4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [],
   "source": [
    "# space = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "#            'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "#            'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "#            'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "#            'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "#            'n_estimators': [100, 500, 1000]}\n",
    "\n",
    "#np.random.seed(1)\n",
    "\n",
    "# Grid Search\n",
    "#xgbc_gs = RandomizedSearchCV(xgb.XGBClassifier(seed = 1), space, scoring='roc_auc', n_iter=25)\n",
    "\n",
    "# Fit Model\n",
    "#results = xgbc_gs.fit(X_train[used_vars],y_train.values.ravel())\n",
    "    \n",
    "# report performance\n",
    "#print('Config: %s' % results.best_params_)\n",
    "#xgbc_gs_perf = performance(xgbc_gs, 'Gradient Boost GS')\n",
    "#xgbc_gs_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Gradient Boosting\n",
    "np.random.seed(1)\n",
    "xgbc_gs = xgb.XGBClassifier(subsample = 0.5, \n",
    "                                n_estimators = 100, \n",
    "                                max_depth = 10, \n",
    "                                learning_rate = 0.01, \n",
    "                                colsample_bytree = 0.8999999999999999, \n",
    "                                colsample_bylevel = 0.8999999999999999).fit(X_train[used_vars], y_train)\n",
    "\n",
    "# Show performance\n",
    "xgbc_gs_perf = performance(xgbc_gs, 'Gradient Boosting GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting 2\n",
    "np.random.seed(1)\n",
    "xgbc_gs2 = xgb.XGBClassifier(subsample = 0.7, \n",
    "                                n_estimators = 100, \n",
    "                                max_depth = 15, \n",
    "                                learning_rate = 0.01, \n",
    "                                colsample_bytree = 0.4, \n",
    "                                colsample_bylevel = 0.7).fit(X_train[used_vars], y_train)\n",
    "\n",
    "# Show performance\n",
    "xgbc_gs2_perf = performance(xgbc_gs2, 'Gradient Boosting GS2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyerparam Grid Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Test_AUC</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N8</th>\n",
       "      <td>0.793659</td>\n",
       "      <td>0.900500</td>\n",
       "      <td>0.801578</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N9</th>\n",
       "      <td>0.793211</td>\n",
       "      <td>0.900750</td>\n",
       "      <td>0.801551</td>\n",
       "      <td>0.90100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N4</th>\n",
       "      <td>0.793595</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>0.801447</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N5</th>\n",
       "      <td>0.793965</td>\n",
       "      <td>0.900312</td>\n",
       "      <td>0.801273</td>\n",
       "      <td>0.90050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.793496</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>0.801217</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.799968</td>\n",
       "      <td>0.904500</td>\n",
       "      <td>0.801207</td>\n",
       "      <td>0.90125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N1</th>\n",
       "      <td>0.793932</td>\n",
       "      <td>0.900500</td>\n",
       "      <td>0.801040</td>\n",
       "      <td>0.90050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N3</th>\n",
       "      <td>0.794021</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>0.800724</td>\n",
       "      <td>0.90050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression GS2</th>\n",
       "      <td>0.792774</td>\n",
       "      <td>0.900250</td>\n",
       "      <td>0.800490</td>\n",
       "      <td>0.90000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N6</th>\n",
       "      <td>0.794175</td>\n",
       "      <td>0.900563</td>\n",
       "      <td>0.800293</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N2</th>\n",
       "      <td>0.794025</td>\n",
       "      <td>0.900563</td>\n",
       "      <td>0.799917</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N7</th>\n",
       "      <td>0.794011</td>\n",
       "      <td>0.900625</td>\n",
       "      <td>0.799843</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression GS3</th>\n",
       "      <td>0.791881</td>\n",
       "      <td>0.900250</td>\n",
       "      <td>0.798724</td>\n",
       "      <td>0.90075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting GS</th>\n",
       "      <td>0.796966</td>\n",
       "      <td>0.905312</td>\n",
       "      <td>0.797519</td>\n",
       "      <td>0.90225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting GS2</th>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.903062</td>\n",
       "      <td>0.796795</td>\n",
       "      <td>0.90050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting GS2</th>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.903062</td>\n",
       "      <td>0.796795</td>\n",
       "      <td>0.90050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression GS</th>\n",
       "      <td>0.788845</td>\n",
       "      <td>0.899687</td>\n",
       "      <td>0.794068</td>\n",
       "      <td>0.90050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest GS3</th>\n",
       "      <td>0.790440</td>\n",
       "      <td>0.899937</td>\n",
       "      <td>0.790877</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest GS2</th>\n",
       "      <td>0.790115</td>\n",
       "      <td>0.899813</td>\n",
       "      <td>0.790774</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest GS4</th>\n",
       "      <td>0.790205</td>\n",
       "      <td>0.899937</td>\n",
       "      <td>0.790554</td>\n",
       "      <td>0.90025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest GS</th>\n",
       "      <td>0.787618</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.785603</td>\n",
       "      <td>0.90075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.838477</td>\n",
       "      <td>0.919687</td>\n",
       "      <td>0.783546</td>\n",
       "      <td>0.89525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.779126</td>\n",
       "      <td>0.905312</td>\n",
       "      <td>0.750977</td>\n",
       "      <td>0.89525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.705586</td>\n",
       "      <td>0.905250</td>\n",
       "      <td>0.681501</td>\n",
       "      <td>0.90225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Train_AUC  Train_Accuracy  Test_AUC  Test_Accuracy\n",
       "Logistic Regression N8    0.793659        0.900500  0.801578        0.90025\n",
       "Logistic Regression N9    0.793211        0.900750  0.801551        0.90100\n",
       "Logistic Regression N4    0.793595        0.900438  0.801447        0.90025\n",
       "Logistic Regression N5    0.793965        0.900312  0.801273        0.90050\n",
       "Logistic Regression       0.793496        0.900438  0.801217        0.90025\n",
       "Gradient Boosting         0.799968        0.904500  0.801207        0.90125\n",
       "Logistic Regression N1    0.793932        0.900500  0.801040        0.90050\n",
       "Logistic Regression N3    0.794021        0.900438  0.800724        0.90050\n",
       "Logistic Regression GS2   0.792774        0.900250  0.800490        0.90000\n",
       "Logistic Regression N6    0.794175        0.900563  0.800293        0.90025\n",
       "Logistic Regression N2    0.794025        0.900563  0.799917        0.90025\n",
       "Logistic Regression N7    0.794011        0.900625  0.799843        0.90025\n",
       "Logistic Regression GS3   0.791881        0.900250  0.798724        0.90075\n",
       "Gradient Boosting GS      0.796966        0.905312  0.797519        0.90225\n",
       "Gradient Boosting GS2     0.795238        0.903062  0.796795        0.90050\n",
       "Gradient Boosting GS2     0.795238        0.903062  0.796795        0.90050\n",
       "Logistic Regression GS    0.788845        0.899687  0.794068        0.90050\n",
       "Random Forest GS3         0.790440        0.899937  0.790877        0.90025\n",
       "Random Forest GS2         0.790115        0.899813  0.790774        0.90025\n",
       "Random Forest GS4         0.790205        0.899937  0.790554        0.90025\n",
       "Random Forest GS          0.787618        0.899500  0.785603        0.90075\n",
       "Random Forest             0.838477        0.919687  0.783546        0.89525\n",
       "KNN                       0.779126        0.905312  0.750977        0.89525\n",
       "SVM                       0.705586        0.905250  0.681501        0.90225"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full overview\n",
    "perf_gs = pd.concat([perf_overview, xgbc_gs_perf, rf_gs_perf, lr_gs_perf, lr_perf_nour1, lr_perf_nour2,\n",
    "                     rf_gs2_perf, lr_perf_nour3, lr_perf_nour4, rf_gs3_perf, xgbc_gs2_perf, rf_gs4_perf, \n",
    "                     xgbc_gs2_perf, lr_perf_gs2, lr_perf_gs3, lr_perf_nour5, lr_perf_nour6, lr_perf_nour7, \n",
    "                     lr_perf_nour8, lr_perf_nour9]).sort_values(by=\"Test_AUC\", ascending=False)\n",
    "perf_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 K-fold Cross-Validation\n",
    "We choose the top 6 models by Test AUC in order to cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfoldcv(classifier, table):\n",
    "    \"\"\"\n",
    "    This Function will help us crossvalidate and add the mean AUC and \n",
    "    Standard Deviation to our table in order to compare later.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #setup cross validation\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10)\n",
    "\n",
    "    #Cross-validate model\n",
    "    scores = cross_val_score(classifier, X_train[used_vars], \n",
    "                        y_train.values.ravel(), \n",
    "                        scoring='roc_auc', \n",
    "                        cv=cv, \n",
    "                        n_jobs=-1)\n",
    "    # report performance\n",
    "    table['CV_AUC'] = np.mean(scores)\n",
    "    table['CV_SD'] = np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crossvalidate everything\n",
    "\n",
    "#Original Models\n",
    "kfoldcv(lr, lr_perf)\n",
    "kfoldcv(xgbc, xgbc_perf)\n",
    "kfoldcv(rf, rf_perf)\n",
    "kfoldcv(knn, knn_perf)\n",
    "kfoldcv(svm, knn_perf)\n",
    "\n",
    "#Boosted Tree Grid Search\n",
    "kfoldcv(xgbc_gs, xgbc_gs_perf)\n",
    "kfoldcv(xgbc_gs2, xgbc_gs2_perf)\n",
    "\n",
    "#Random Forest Grid Search\n",
    "kfoldcv(rf_gs, rf_gs_perf)\n",
    "kfoldcv(rf_gs2, rf_gs2_perf)\n",
    "kfoldcv(rf_gs3, rf_gs3_perf)\n",
    "kfoldcv(rf_gs4, rf_gs4_perf)\n",
    "\n",
    "#Logistic Grid Search\n",
    "kfoldcv(lr_gs, lr_gs_perf)\n",
    "kfoldcv(lr_gs2, lr_perf_gs2)\n",
    "kfoldcv(lr_gs2, lr_perf_gs3)\n",
    "kfoldcv(lr_nour1, lr_perf_nour1)\n",
    "kfoldcv(lr_nour2, lr_perf_nour2)\n",
    "kfoldcv(lr_nour3, lr_perf_nour3)\n",
    "kfoldcv(lr_nour4, lr_perf_nour4)\n",
    "kfoldcv(lr_nour5, lr_perf_nour5)\n",
    "kfoldcv(lr_nour6, lr_perf_nour6)\n",
    "kfoldcv(lr_nour7, lr_perf_nour7)\n",
    "kfoldcv(lr_nour8, lr_perf_nour8)\n",
    "kfoldcv(lr_nour9, lr_perf_nour9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of Cross-Validation\n",
    "perf_cv = pd.concat([xgbc_perf, rf_perf, lr_perf, knn_perf, svm_perf, \n",
    "                    xgbc_gs_perf, xgbc_gs2_perf, rf_gs_perf, \n",
    "                    lr_gs_perf, lr_perf_gs2, lr_perf_gs3, lr_perf_nour1, lr_perf_nour2,\n",
    "                    lr_perf_nour3, lr_perf_nour4, lr_perf_nour5, lr_perf_nour6,\n",
    "                    lr_perf_nour7, lr_perf_nour8, lr_perf_nour9, \n",
    "                    rf_gs2_perf, rf_gs3_perf, rf_gs4_perf]).sort_values(by=\"CV_AUC\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Create New Train Set\n",
    "We re-concatenate the whole train dataset to fit our models with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new training set \n",
    "new_train = pd.concat([X_train[used_vars], X_test[used_vars]])\n",
    "new_target = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_perf(classifier, table):\n",
    "    \"\"\"\n",
    "    This function returns the performance of AUC and Accuracy for the specified model.\n",
    "    \"model\" represents the name of the model function, and \"printname\" is a string to give the model a name in the overview table. \n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "\n",
    "    probabilities = pd.DataFrame(classifier.predict_proba(new_train))[1]\n",
    "    auc           = roc_auc_score(np.array(new_target),np.array(probabilities))\n",
    "    table['New_Train_AUC'] = auc\n",
    "    return table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression \n",
    "lr = LogisticRegression().fit(new_train, new_target)\n",
    "lr_perf = new_perf(lr, lr_perf)\n",
    "\n",
    "lr_gs=LogisticRegression(C = 0.019306977288832496, penalty = 'l2').fit(new_train, new_target)\n",
    "lr_gs_perf = new_perf(lr, lr_gs_perf)\n",
    "\n",
    "lr_gs2=LogisticRegression(C= 0.2442053094548655, penalty= 'l2', solver = 'lbfgs').fit(new_train, new_target)\n",
    "lr_perf_gs2 = new_perf(lr_gs2, lr_perf_gs2)\n",
    "\n",
    "lr_gs3=LogisticRegression(C= 0.09651957137350628, penalty= 'l2', solver = 'lbfgs').fit(new_train, new_target)\n",
    "lr_perf_gs3 = new_perf(lr_gs3, lr_perf_gs3)\n",
    "\n",
    "lr_nour1=LogisticRegression(C= 46.41588833612777, penalty= 'l2', solver = 'lbfgs').fit(new_train, new_target)\n",
    "lr_perf_nour1 = new_perf(lr_nour1, lr_perf_nour1)\n",
    "\n",
    "lr_nour2=LogisticRegression(C= 40960000.0, penalty= 'l2', solver = 'lbfgs').fit(new_train, new_target)\n",
    "lr_perf_nour2 = new_perf(lr_nour2, lr_perf_nour2)\n",
    "\n",
    "lr_nour3 = LogisticRegression(C= 62.505519252739695, penalty= 'l2', solver = 'lbfgs').fit(new_train, new_target)\n",
    "lr_perf_nour3 = new_perf(lr_nour3, lr_perf_nour3)\n",
    "\n",
    "lr_nour4 = LogisticRegression(C=  1.2589254117941673, penalty= 'l2', solver = 'newton-cg').fit(new_train, new_target)\n",
    "lr_perf_nour4 = new_perf(lr_nour4, lr_perf_nour4)\n",
    "\n",
    "lr_nour5 = LogisticRegression(C=  11.513953993264476, penalty= 'l2', solver = 'lbfgs').fit(new_train, new_target)\n",
    "lr_perf_nour5 = new_perf(lr_nour5, lr_perf_nour5)\n",
    "\n",
    "lr_nour6=LogisticRegression(C=  1e+20, penalty= 'l2', solver = 'lbfgs').fit(new_train, new_target)\n",
    "lr_perf_nour6 = new_perf(lr_nour6, lr_perf_nour6)\n",
    "\n",
    "lr_nour7=LogisticRegression(C=  640884341.8931684, penalty= 'l2', solver = 'lbfgs').fit(new_train, new_target)\n",
    "lr_perf_nour7 = new_perf(lr_nour7, lr_perf_nour7)\n",
    "\n",
    "lr_nour8 = LogisticRegression(C=  2.3741612048163567, penalty= 'l1', solver = 'liblinear').fit(new_train, new_target)\n",
    "lr_perf_nour8 = new_perf(lr_nour8, lr_perf_nour8)\n",
    "\n",
    "lr_nour9=LogisticRegression(C=  1.2589254117941673, penalty= 'l1', solver = 'liblinear').fit(new_train, new_target)\n",
    "lr_perf_nour9 = new_perf(lr_nour9, lr_perf_nour9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted Tree\n",
    "xgbc = GradientBoostingClassifier().fit(new_train, new_target)\n",
    "xgbc_perf = new_perf(xgbc, xgbc_perf)\n",
    "\n",
    "xgbc_gs = xgb.XGBClassifier(subsample = 0.5, \n",
    "                                n_estimators = 100, \n",
    "                                max_depth = 10, \n",
    "                                learning_rate = 0.01, \n",
    "                                colsample_bytree = 0.8999999999999999, \n",
    "                                colsample_bylevel = 0.8999999999999999).fit(new_train, new_target)\n",
    "xgbc_gs_perf = new_perf(xgbc_gs, xgbc_gs_perf)\n",
    "\n",
    "xgbc_gs2 = xgb.XGBClassifier(subsample = 0.7, \n",
    "                                n_estimators = 100, \n",
    "                                max_depth = 15, \n",
    "                                learning_rate = 0.01, \n",
    "                                colsample_bytree = 0.4, \n",
    "                                colsample_bylevel = 0.7).fit(new_train, new_target)\n",
    "xgbc_gs2_perf = new_perf(xgbc_gs2, xgbc_gs2_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-635-f5033760e32a>:1: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "\n",
      "<ipython-input-635-f5033760e32a>:9: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "\n",
      "<ipython-input-635-f5033760e32a>:16: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "\n",
      "<ipython-input-635-f5033760e32a>:22: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_gs=RandomForestClassifier(n_estimators = 579, \n",
    "                        min_samples_split = 10, \n",
    "                        min_samples_leaf = 4, \n",
    "                        max_features = 'auto', \n",
    "                        max_depth = 5, \n",
    "                        bootstrap = True).fit(new_train, new_target)\n",
    "rf_gs_perf = new_perf(rf_gs, rf_gs_perf)\n",
    "\n",
    "rf_gs2=RandomForestClassifier(n_estimators = 500, \n",
    "                        criterion = 'entropy',\n",
    "                        max_features = 'sqrt', \n",
    "                        max_depth = 5, \n",
    "                        bootstrap = True).fit(new_train, new_target)\n",
    "rf_gs2_perf = new_perf(rf_gs2, rf_gs2_perf)\n",
    "\n",
    "rf_gs3=RandomForestClassifier(n_estimators = 1000, \n",
    "                        criterion = 'entropy',\n",
    "                        max_features = 'auto', \n",
    "                        max_depth = 5).fit(new_train, new_target)\n",
    "rf_gs3_perf = new_perf(rf_gs3, rf_gs3_perf)\n",
    "\n",
    "rf_gs4=RandomForestClassifier(n_estimators = 2000, \n",
    "                        criterion = 'entropy',\n",
    "                        max_features = 'auto', \n",
    "                        max_depth = 5, \n",
    "                        bootstrap = True).fit(new_train, new_target) \n",
    "rf_gs4_perf = new_perf(rf_gs4, rf_gs4_perf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Cross-Validate New Train AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfoldcv_new(classifier, table):\n",
    "    \"\"\"\n",
    "    This Function will help us crossvalidate and add the mean AUC and \n",
    "    Standard Deviation to our table in order to compare later.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    #setup cross validation\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10)\n",
    "\n",
    "    #Cross-validate model\n",
    "    scores = cross_val_score(classifier, new_train, \n",
    "                        new_target, \n",
    "                        scoring='roc_auc', \n",
    "                        cv=cv, \n",
    "                        n_jobs=-1)\n",
    "    # report performance\n",
    "    table['New_CV_AUC'] = np.mean(scores)\n",
    "    table['New_CV_SD'] = np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crossvalidate everything\n",
    "\n",
    "#Original Models\n",
    "kfoldcv_new(lr, lr_perf)\n",
    "kfoldcv_new(xgbc, xgbc_perf)\n",
    "kfoldcv_new(rf, rf_perf)\n",
    "kfoldcv_new(knn, knn_perf)\n",
    "kfoldcv_new(svm, knn_perf)\n",
    "\n",
    "#Boosted Tree Grid Search\n",
    "kfoldcv_new(xgbc_gs, xgbc_gs_perf)\n",
    "kfoldcv_new(xgbc_gs2, xgbc_gs2_perf)\n",
    "\n",
    "#Random Forest Grid Search\n",
    "kfoldcv_new(rf_gs, rf_gs_perf)\n",
    "kfoldcv_new(rf_gs2, rf_gs2_perf)\n",
    "kfoldcv_new(rf_gs3, rf_gs3_perf)\n",
    "kfoldcv_new(rf_gs4, rf_gs4_perf)\n",
    "\n",
    "#Logistic Grid Search\n",
    "kfoldcv_new(lr_gs, lr_gs_perf)\n",
    "kfoldcv_new(lr_gs2, lr_perf_gs2)\n",
    "kfoldcv_new(lr_gs2, lr_perf_gs3)\n",
    "kfoldcv_new(lr_nour1, lr_perf_nour1)\n",
    "kfoldcv_new(lr_nour2, lr_perf_nour2)\n",
    "kfoldcv_new(lr_nour3, lr_perf_nour3)\n",
    "kfoldcv_new(lr_nour4, lr_perf_nour4)\n",
    "kfoldcv_new(lr_nour5, lr_perf_nour5)\n",
    "kfoldcv_new(lr_nour6, lr_perf_nour6)\n",
    "kfoldcv_new(lr_nour7, lr_perf_nour7)\n",
    "kfoldcv_new(lr_nour8, lr_perf_nour8)\n",
    "kfoldcv_new(lr_nour9, lr_perf_nour9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train_AUC</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Test_AUC</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>CV_AUC</th>\n",
       "      <th>CV_SD</th>\n",
       "      <th>New_Train_AUC</th>\n",
       "      <th>New_CV_AUC</th>\n",
       "      <th>New_CV_SD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N4</th>\n",
       "      <td>0.793595</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>0.801447</td>\n",
       "      <td>0.90025</td>\n",
       "      <td>0.788581</td>\n",
       "      <td>0.019085</td>\n",
       "      <td>0.795647</td>\n",
       "      <td>0.791383</td>\n",
       "      <td>0.018620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.793496</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>0.801217</td>\n",
       "      <td>0.90025</td>\n",
       "      <td>0.788544</td>\n",
       "      <td>0.019078</td>\n",
       "      <td>0.795733</td>\n",
       "      <td>0.791376</td>\n",
       "      <td>0.018655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N5</th>\n",
       "      <td>0.793965</td>\n",
       "      <td>0.900312</td>\n",
       "      <td>0.801273</td>\n",
       "      <td>0.90050</td>\n",
       "      <td>0.788252</td>\n",
       "      <td>0.019056</td>\n",
       "      <td>0.795973</td>\n",
       "      <td>0.791301</td>\n",
       "      <td>0.018584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N8</th>\n",
       "      <td>0.793659</td>\n",
       "      <td>0.900500</td>\n",
       "      <td>0.801578</td>\n",
       "      <td>0.90025</td>\n",
       "      <td>0.788439</td>\n",
       "      <td>0.019046</td>\n",
       "      <td>0.795535</td>\n",
       "      <td>0.791266</td>\n",
       "      <td>0.018690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N9</th>\n",
       "      <td>0.793211</td>\n",
       "      <td>0.900750</td>\n",
       "      <td>0.801551</td>\n",
       "      <td>0.90100</td>\n",
       "      <td>0.788589</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>0.795143</td>\n",
       "      <td>0.791245</td>\n",
       "      <td>0.018705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N1</th>\n",
       "      <td>0.793932</td>\n",
       "      <td>0.900500</td>\n",
       "      <td>0.801040</td>\n",
       "      <td>0.90050</td>\n",
       "      <td>0.788126</td>\n",
       "      <td>0.019074</td>\n",
       "      <td>0.795725</td>\n",
       "      <td>0.791202</td>\n",
       "      <td>0.018569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N3</th>\n",
       "      <td>0.794021</td>\n",
       "      <td>0.900438</td>\n",
       "      <td>0.800724</td>\n",
       "      <td>0.90050</td>\n",
       "      <td>0.788122</td>\n",
       "      <td>0.019020</td>\n",
       "      <td>0.796021</td>\n",
       "      <td>0.791183</td>\n",
       "      <td>0.018594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N7</th>\n",
       "      <td>0.794011</td>\n",
       "      <td>0.900625</td>\n",
       "      <td>0.799843</td>\n",
       "      <td>0.90025</td>\n",
       "      <td>0.788053</td>\n",
       "      <td>0.019090</td>\n",
       "      <td>0.795772</td>\n",
       "      <td>0.791179</td>\n",
       "      <td>0.018559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N6</th>\n",
       "      <td>0.794175</td>\n",
       "      <td>0.900563</td>\n",
       "      <td>0.800293</td>\n",
       "      <td>0.90025</td>\n",
       "      <td>0.788061</td>\n",
       "      <td>0.019075</td>\n",
       "      <td>0.795759</td>\n",
       "      <td>0.791145</td>\n",
       "      <td>0.018573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression N2</th>\n",
       "      <td>0.794025</td>\n",
       "      <td>0.900563</td>\n",
       "      <td>0.799917</td>\n",
       "      <td>0.90025</td>\n",
       "      <td>0.788108</td>\n",
       "      <td>0.019060</td>\n",
       "      <td>0.796033</td>\n",
       "      <td>0.791139</td>\n",
       "      <td>0.018549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression GS2</th>\n",
       "      <td>0.792774</td>\n",
       "      <td>0.900250</td>\n",
       "      <td>0.800490</td>\n",
       "      <td>0.90000</td>\n",
       "      <td>0.788322</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>0.794735</td>\n",
       "      <td>0.791034</td>\n",
       "      <td>0.018758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression GS3</th>\n",
       "      <td>0.791881</td>\n",
       "      <td>0.900250</td>\n",
       "      <td>0.798724</td>\n",
       "      <td>0.90075</td>\n",
       "      <td>0.788322</td>\n",
       "      <td>0.019152</td>\n",
       "      <td>0.793937</td>\n",
       "      <td>0.791034</td>\n",
       "      <td>0.018758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.799968</td>\n",
       "      <td>0.904500</td>\n",
       "      <td>0.801207</td>\n",
       "      <td>0.90125</td>\n",
       "      <td>0.787352</td>\n",
       "      <td>0.018754</td>\n",
       "      <td>0.800438</td>\n",
       "      <td>0.790534</td>\n",
       "      <td>0.018160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting GS2</th>\n",
       "      <td>0.795238</td>\n",
       "      <td>0.903062</td>\n",
       "      <td>0.796795</td>\n",
       "      <td>0.90050</td>\n",
       "      <td>0.787991</td>\n",
       "      <td>0.019086</td>\n",
       "      <td>0.795807</td>\n",
       "      <td>0.789560</td>\n",
       "      <td>0.018206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting GS</th>\n",
       "      <td>0.796966</td>\n",
       "      <td>0.905312</td>\n",
       "      <td>0.797519</td>\n",
       "      <td>0.90225</td>\n",
       "      <td>0.787508</td>\n",
       "      <td>0.019275</td>\n",
       "      <td>0.799299</td>\n",
       "      <td>0.789306</td>\n",
       "      <td>0.017907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression GS</th>\n",
       "      <td>0.788845</td>\n",
       "      <td>0.899687</td>\n",
       "      <td>0.794068</td>\n",
       "      <td>0.90050</td>\n",
       "      <td>0.786832</td>\n",
       "      <td>0.019085</td>\n",
       "      <td>0.795733</td>\n",
       "      <td>0.788655</td>\n",
       "      <td>0.018951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest GS4</th>\n",
       "      <td>0.790205</td>\n",
       "      <td>0.899937</td>\n",
       "      <td>0.790554</td>\n",
       "      <td>0.90025</td>\n",
       "      <td>0.783766</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>0.790280</td>\n",
       "      <td>0.785079</td>\n",
       "      <td>0.018306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest GS3</th>\n",
       "      <td>0.790440</td>\n",
       "      <td>0.899937</td>\n",
       "      <td>0.790877</td>\n",
       "      <td>0.90025</td>\n",
       "      <td>0.783696</td>\n",
       "      <td>0.018796</td>\n",
       "      <td>0.790549</td>\n",
       "      <td>0.785059</td>\n",
       "      <td>0.018237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest GS2</th>\n",
       "      <td>0.790115</td>\n",
       "      <td>0.899813</td>\n",
       "      <td>0.790774</td>\n",
       "      <td>0.90025</td>\n",
       "      <td>0.783472</td>\n",
       "      <td>0.018814</td>\n",
       "      <td>0.790587</td>\n",
       "      <td>0.784726</td>\n",
       "      <td>0.018497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest GS</th>\n",
       "      <td>0.787618</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.785603</td>\n",
       "      <td>0.90075</td>\n",
       "      <td>0.783156</td>\n",
       "      <td>0.018917</td>\n",
       "      <td>0.787064</td>\n",
       "      <td>0.783517</td>\n",
       "      <td>0.018267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.838477</td>\n",
       "      <td>0.919687</td>\n",
       "      <td>0.783546</td>\n",
       "      <td>0.89525</td>\n",
       "      <td>0.757773</td>\n",
       "      <td>0.022136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.765274</td>\n",
       "      <td>0.019646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Train_AUC  Train_Accuracy  Test_AUC  Test_Accuracy  \\\n",
       "Logistic Regression N4    0.793595        0.900438  0.801447        0.90025   \n",
       "Logistic Regression       0.793496        0.900438  0.801217        0.90025   \n",
       "Logistic Regression N5    0.793965        0.900312  0.801273        0.90050   \n",
       "Logistic Regression N8    0.793659        0.900500  0.801578        0.90025   \n",
       "Logistic Regression N9    0.793211        0.900750  0.801551        0.90100   \n",
       "Logistic Regression N1    0.793932        0.900500  0.801040        0.90050   \n",
       "Logistic Regression N3    0.794021        0.900438  0.800724        0.90050   \n",
       "Logistic Regression N7    0.794011        0.900625  0.799843        0.90025   \n",
       "Logistic Regression N6    0.794175        0.900563  0.800293        0.90025   \n",
       "Logistic Regression N2    0.794025        0.900563  0.799917        0.90025   \n",
       "Logistic Regression GS2   0.792774        0.900250  0.800490        0.90000   \n",
       "Logistic Regression GS3   0.791881        0.900250  0.798724        0.90075   \n",
       "Gradient Boosting         0.799968        0.904500  0.801207        0.90125   \n",
       "Gradient Boosting GS2     0.795238        0.903062  0.796795        0.90050   \n",
       "Gradient Boosting GS      0.796966        0.905312  0.797519        0.90225   \n",
       "Logistic Regression GS    0.788845        0.899687  0.794068        0.90050   \n",
       "Random Forest GS4         0.790205        0.899937  0.790554        0.90025   \n",
       "Random Forest GS3         0.790440        0.899937  0.790877        0.90025   \n",
       "Random Forest GS2         0.790115        0.899813  0.790774        0.90025   \n",
       "Random Forest GS          0.787618        0.899500  0.785603        0.90075   \n",
       "Random Forest             0.838477        0.919687  0.783546        0.89525   \n",
       "\n",
       "                           CV_AUC     CV_SD  New_Train_AUC  New_CV_AUC  \\\n",
       "Logistic Regression N4   0.788581  0.019085       0.795647    0.791383   \n",
       "Logistic Regression      0.788544  0.019078       0.795733    0.791376   \n",
       "Logistic Regression N5   0.788252  0.019056       0.795973    0.791301   \n",
       "Logistic Regression N8   0.788439  0.019046       0.795535    0.791266   \n",
       "Logistic Regression N9   0.788589  0.019101       0.795143    0.791245   \n",
       "Logistic Regression N1   0.788126  0.019074       0.795725    0.791202   \n",
       "Logistic Regression N3   0.788122  0.019020       0.796021    0.791183   \n",
       "Logistic Regression N7   0.788053  0.019090       0.795772    0.791179   \n",
       "Logistic Regression N6   0.788061  0.019075       0.795759    0.791145   \n",
       "Logistic Regression N2   0.788108  0.019060       0.796033    0.791139   \n",
       "Logistic Regression GS2  0.788322  0.019152       0.794735    0.791034   \n",
       "Logistic Regression GS3  0.788322  0.019152       0.793937    0.791034   \n",
       "Gradient Boosting        0.787352  0.018754       0.800438    0.790534   \n",
       "Gradient Boosting GS2    0.787991  0.019086       0.795807    0.789560   \n",
       "Gradient Boosting GS     0.787508  0.019275       0.799299    0.789306   \n",
       "Logistic Regression GS   0.786832  0.019085       0.795733    0.788655   \n",
       "Random Forest GS4        0.783766  0.018950       0.790280    0.785079   \n",
       "Random Forest GS3        0.783696  0.018796       0.790549    0.785059   \n",
       "Random Forest GS2        0.783472  0.018814       0.790587    0.784726   \n",
       "Random Forest GS         0.783156  0.018917       0.787064    0.783517   \n",
       "Random Forest            0.757773  0.022136            NaN    0.765274   \n",
       "\n",
       "                         New_CV_SD  \n",
       "Logistic Regression N4    0.018620  \n",
       "Logistic Regression       0.018655  \n",
       "Logistic Regression N5    0.018584  \n",
       "Logistic Regression N8    0.018690  \n",
       "Logistic Regression N9    0.018705  \n",
       "Logistic Regression N1    0.018569  \n",
       "Logistic Regression N3    0.018594  \n",
       "Logistic Regression N7    0.018559  \n",
       "Logistic Regression N6    0.018573  \n",
       "Logistic Regression N2    0.018549  \n",
       "Logistic Regression GS2   0.018758  \n",
       "Logistic Regression GS3   0.018758  \n",
       "Gradient Boosting         0.018160  \n",
       "Gradient Boosting GS2     0.018206  \n",
       "Gradient Boosting GS      0.017907  \n",
       "Logistic Regression GS    0.018951  \n",
       "Random Forest GS4         0.018306  \n",
       "Random Forest GS3         0.018237  \n",
       "Random Forest GS2         0.018497  \n",
       "Random Forest GS          0.018267  \n",
       "Random Forest             0.019646  "
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Visualization of final results\n",
    "perf_cv_final = pd.concat([xgbc_perf, rf_perf, lr_perf,\n",
    "                    xgbc_gs_perf, xgbc_gs2_perf, rf_gs_perf, \n",
    "                    lr_gs_perf, lr_perf_gs2, lr_perf_gs3, lr_perf_nour1, lr_perf_nour2,\n",
    "                    lr_perf_nour3, lr_perf_nour4, lr_perf_nour5, lr_perf_nour6,\n",
    "                    lr_perf_nour7, lr_perf_nour8, lr_perf_nour9, \n",
    "                    rf_gs2_perf, rf_gs3_perf, rf_gs4_perf]).sort_values(by=\"New_CV_AUC\", ascending=False)\n",
    "perf_cv_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make prediction\n",
    "\n",
    "Use the best model to make prediction on test set. Submit the result to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare predictions for submission\n",
    "probabilities = pd.DataFrame(lr.predict_proba(test[used_vars]))[1]\n",
    "submission = pd.DataFrame(test['client_id'])\n",
    "submission['subscribe'] = probabilities\n",
    "\n",
    "#Export file\n",
    "submission.to_csv(wd + \"/data/submissions/lr.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare predictions for submission\n",
    "probabilities = pd.DataFrame(lr_nour2.predict_proba(test[used_vars]))[1]\n",
    "submission = pd.DataFrame(test['client_id'])\n",
    "submission['subscribe'] = probabilities\n",
    "\n",
    "#Export file\n",
    "submission.to_csv(wd + \"/data/submissions/lrnour2.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare predictions for submission\n",
    "probabilities = pd.DataFrame(xgbc.predict_proba(test[used_vars]))[1]\n",
    "submission = pd.DataFrame(test['client_id'])\n",
    "submission['subscribe'] = probabilities\n",
    "\n",
    "#Export file\n",
    "submission.to_csv(wd + \"/data/submissions/xgbc.csv\", index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare predictions for submission\n",
    "probabilities = pd.DataFrame(lr_nour4.predict_proba(test[used_vars]))[1]\n",
    "submission = pd.DataFrame(test['client_id'])\n",
    "submission['subscribe'] = probabilities\n",
    "\n",
    "#Export file\n",
    "submission.to_csv(wd + \"/data/submissions/lrnour4.csv\", index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
